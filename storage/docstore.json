{"docstore/metadata": {"68ec127c-3368-45d8-ba12-1ab8d1b5da7f": {"doc_hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58"}, "e76227a0-1079-4add-9293-ebba4cae6936": {"doc_hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12"}, "8578f920-f316-4f3c-a19e-4e59b34702f2": {"doc_hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0"}, "afbb1e39-030f-40e0-a814-4d2a86aade92": {"doc_hash": "befdea40f58666672cf58a2bc147205d5c90995371a371c724b76ab77fe0ca21", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56": {"doc_hash": "408dae0cd06bd88e4163f047edd9f950d5b5881da23a470e3633d8e6b48ca860", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "ea768f27-d3a3-41f5-9356-26a754f22c1a": {"doc_hash": "034806a77fa92a77852393f06256f741951a18a6dacc5ed7bb50f964a89c9f67", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "58d3d8b7-fad9-4ffe-9973-e328f735883a": {"doc_hash": "54f7ac83369b82a987221788ab592fe8168536428b716bc80821d390b7fc15d3", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "f4f61817-9940-491f-a132-58400ea21b8e": {"doc_hash": "751566c7bee7d80b12b9c794a6755b6673ee4d72e3f1a36db8396273b51e9667", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "2f4605df-f31e-449e-92d1-93d1274a727e": {"doc_hash": "c34cbc9a4eac70c885ca0ae5355d563a9a3707c6903c198b2cea5cfa98a37afc", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "e5a722ad-ad75-4912-bb0f-12a236644ffd": {"doc_hash": "571a7291e2c6a2a7cb465518cb20b5e843adeba75ad65daf3cb9850d795b7510", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "23b822d0-f03d-4d55-9707-a849a754097d": {"doc_hash": "6eb0c89096ac4ebc17413e035fce15dcb221ec374598db05217bfc334c032423", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "3fe5bdb3-cb30-4d98-bf62-5be46d14ae7d": {"doc_hash": "75c4127fab454b54c1d8203610abb5e8d72f8172e94e4539ee8ce8438651cf3a", "ref_doc_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f"}, "4854438e-115d-43ce-8e87-5c2e18dedd86": {"doc_hash": "b6fb0e6309c236137925353e433e3e866023f6213fc8b34cd736c7afd13492a1", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "178ab083-1bba-4829-8bac-8e9f33656769": {"doc_hash": "5ad4e0eccf33563f7e43a4baa97950d82768734c87eeae154666cb5c1a8e8ee9", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "ef6162f4-0792-4c66-830f-be31e5eceef7": {"doc_hash": "809d60401e53b95a52912e9be215f7459d21cc5a3824b78ab02714666dec0f71", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "055ccaa7-9790-4892-978c-888419b70a49": {"doc_hash": "27f20436580c44cbbe642d1c627bf45e5be3c48ed0bae79aff22e77fd0e99906", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "41bf9b4f-31e6-4c0b-9777-9262ea2c447e": {"doc_hash": "dd55922cbe5809ad327695b840c9814a30cd60a56371e8c1cbac17d1a8778ebb", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "002cf1b1-9963-4b4e-9dc3-d013939dc585": {"doc_hash": "4b4182f91c0495f745da207374c34c3541f4d0bbd74efc2ce2cb4d42f92828d6", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "5c54336c-70bd-4cd5-bff0-48e4773d2172": {"doc_hash": "af6d959a3fc4f7a92d80dc2e4c8cde11f85108f592598adfcfcb2e639a438507", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "381082d5-7242-4bf8-88f0-c4c5bbea87e3": {"doc_hash": "51346e9f823cf5779e75f773dcaf13de3d0165f0a3a420aab09c36e1b3fc2d5e", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "f719ef32-c5f6-492b-b6b4-8de6a89f8d77": {"doc_hash": "525212819141d11ea874f98a10a3bc60f9e24cddf195635f16dc4a90da69ab66", "ref_doc_id": "e76227a0-1079-4add-9293-ebba4cae6936"}, "8a2722fe-d6c2-4cd5-9b74-d730cea19600": {"doc_hash": "c168b0dc00f99ce768034b7bbc3c4724480f72fd41f968a0fa3c2ab16a319215", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "5856ed92-9a46-4e6b-a8af-b9523f877ec8": {"doc_hash": "9dbb151a5ec8dad557ca42f71a229925100249f13f5a78c6ff26d7795e8c030b", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "0bfa74d1-b907-44bf-8f95-c3f406701893": {"doc_hash": "d5313f400a02aaaa998a3d041be6866358b03b132f3af057c7b802d68c16a69f", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "ebb48b40-3514-4324-891c-025a784addee": {"doc_hash": "9cc48d7e27edbe4b1e15950d3a9fae32171e9f07cda0936d993cef0edeeaf96a", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "9cb55855-869d-4ba8-bf0e-8a72b410e8ca": {"doc_hash": "93282f3d34ec6ef42818952b387b4ebb0615d27240b2256127b33c6aff699e4d", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "2d519525-935e-4fda-9d5d-a4b097534a98": {"doc_hash": "7d996d7c82ddc32886d20c6bf0fc9cdd7e084d3627fcf865bbb27bb666c5ad71", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "06308865-04e0-41e4-9337-1eb5aba373a4": {"doc_hash": "7a5e5cdfbf6d967785d60149524f96ea2c41663487bf6225e924478bfb824346", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "26538059-2366-4518-9021-dad98651bf99": {"doc_hash": "8275dc9eb0963a983061021b857a7aa69d0c817fe51cee6cf03efb4272c3f913", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "3ed433f3-d937-46c5-9b65-152e70ee9222": {"doc_hash": "53a9ce076c151fcb919ac9e26b6a1b5eee8afebf276a99915a905a7d1060d4a0", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "60b087a0-10d3-458a-b540-d518291d16d4": {"doc_hash": "e04ff2ba2379c3582531bdc554c518bb25e947d1d0f1287fad0fd82db15acda1", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "8bf81715-2f6f-4b9b-8c23-99008871b88f": {"doc_hash": "0a4c8c5abe4024b4213e37cca35791311da383ecd5c53898d0c49c53667bfd23", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "92d8a9d2-1ef2-4a10-b365-0cd52b29241b": {"doc_hash": "d54794d83d7902642b56c8bed4f3cb252d1ecb2195ea8a5a302a73f4ae20edbf", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}, "797cce33-5648-4afe-83b3-af449a6ae5b7": {"doc_hash": "4067c523611e78e8380a73d4af7fd2458de690601f8753326eff05c8b83d313b", "ref_doc_id": "8578f920-f316-4f3c-a19e-4e59b34702f2"}}, "docstore/data": {"afbb1e39-030f-40e0-a814-4d2a86aade92": {"__data__": {"id_": "afbb1e39-030f-40e0-a814-4d2a86aade92", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56", "node_type": "1", "metadata": {}, "hash": "2afacb09a915656fa4458d27909a999bbda75ca82effc3774d61a8304d3e951d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proceedings of the 53rd Hawaii International Conference on System Sciences | 2020Page 3256URI: https://hdl.handle.net/10125/64139978-0-9981331-3-3(CC BY-NC-ND 4.0)\nBlood Glucose Forecasting using LSTM Variants under the Context of Open Source Artificial Pancreas System\nTianfeng Wang Weizi Li University of Reading University of Reading tianfeng.wang@pgr.reading.ac.uk weizi.li@henley.ac.uk\nAbstract\ndestroyed, preventing the body from being able to produce enough insulin to adequately regulate blood glucose levels [1]. Estimating and predicting blood glucose in both the short-term and long-term are essential for effective management of diabetes. The traditional approach to managing Type 1 diabetes relies on patients\u2019 own estimation of insulin amount which often leads to hyperglycemia or hypoglycemia due to incorrect estimation [2]. The artificial pancreas, or closed-loop insulin delivery system is emerging to continuously monitors blood sugar levels, calculates the amount of insulin required (through a device such as a tablet or mobile phone), and automatically delivers insulin through a pump [3]. Although the insulin pump automatically adjusts basal insulin in existing FDA approved hybrid closed-loop system [4], accurate prediction on long-term blood glucose level under the context of closed-loop artificial pancreas system (APS) is of high importance because it is essential for preventative blood glucose control and to better guide meals intake, exercise and support patients planning daily activities further ahead (e.g. 1 hour). This will allow patients to take actions ahead of time in order to the occurrence of adverse glycaemic events. Existing blood glucose prediction research focuses on short term predictions such as 15 minutes to 30 minutes but the performance of the prediction models dropped dramatically when it comes to long term predictions such as 45 min to 1 hour [5]. The state-of-the-art deep learning models such as long short term memory (LSTM) and its variants demonstrate strong capabilities in long term forecasting [6]. In this research, we aim to develop a long-term blood glucose forecasting model based on convolutional-LSTM and compare our model with other LSTM models and existing methods used in blood glucose prediction.\nHigh accuracy of blood glucose prediction over the long for preventative diabetes management. The emerging closed-loop insulin delivery system such as the artificial pancreas system (APS) provides opportunities for improved glycaemic control for patients with type 1 diabetes. Existing blood glucose studies are proven effective only within 30 minutes but the the accuracy deteriorates drastically when prediction horizon increases to 45 minutes and 60 minutes. Deep learning, especially for long short term memory (LSTM) and its variants have recently been applied in various areas to achieve state-of-the-art results in tasks with complex time series data. In this study, we present deep LSTM based models that are capable of forecasting long term blood glucose levels with improved prediction and clinical accuracy. We evaluate our approach using 20 cases(878,000 glucose values) from Open Source Artificial Pancreas System (OpenAPS). On 30-minutes and 45-minutes prediction, our Stacked-LSTM achieved the best performance with Root-Mean-Square-Error (RMSE) marks 11.96 & 15.81 and Clark-Grid-ZoneA marks 0.887 & 0.784. In terms of 60-minutes prediction, our ConvLSTM has the best performance with RMSE = 19.6 and Clark-Grid- ZoneA=0.714. Our models outperform existing methods in both prediction and clinical accuracy. This research can hopefully support patients with type 1 diabetes to better manage their behavior in a more preventative way and can be used in future real APS context.\nterm\nis essential\n1. Introduction\nType 1 Diabetes is an autoimmune disease that causes the insulin-producing beta cells in the pancreas to be\nPage 3257\nand PH need to be considered to best meet patients\u2019 needs. However, existing research can only demonstrate high performance in 30 min PH but cannot meet the accuracy requirement for glycaemic control for a longer period. Therefore a 30 min PH is the most common value for blood glucose prediction but high accuracy in longer PH is needed. Deep learning, which incorporates methods recently proved the already established methodologies [24]. It has led to significant progress in computer vision [25], disease diagnosis [26], and healthcare [27, 28]. Deep learning shows superior performance to traditional ML techniques due to this ability to automatically learn features with higher complexity and representations [29-32]. Recurrent Neural Networks (RNNs) have shown its capability in many applications with time series or sequential data, including machine translation [33, 34] and speech recognition [35].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56": {"__data__": {"id_": "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afbb1e39-030f-40e0-a814-4d2a86aade92", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "befdea40f58666672cf58a2bc147205d5c90995371a371c724b76ab77fe0ca21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea768f27-d3a3-41f5-9356-26a754f22c1a", "node_type": "1", "metadata": {}, "hash": "b7acde9192aed4e3d8b191d540e689a347140eec5a8c4e383d2877a838be23bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, existing research can only demonstrate high performance in 30 min PH but cannot meet the accuracy requirement for glycaemic control for a longer period. Therefore a 30 min PH is the most common value for blood glucose prediction but high accuracy in longer PH is needed. Deep learning, which incorporates methods recently proved the already established methodologies [24]. It has led to significant progress in computer vision [25], disease diagnosis [26], and healthcare [27, 28]. Deep learning shows superior performance to traditional ML techniques due to this ability to automatically learn features with higher complexity and representations [29-32]. Recurrent Neural Networks (RNNs) have shown its capability in many applications with time series or sequential data, including machine translation [33, 34] and speech recognition [35]. One of the major challenges in designing systems using classical RNNs is their limited capacity to learn long-term dependencies, because of the vanishing or exploding gradient problem [36]. Recent deep RNNs incorporate mechanisms to address this problem [37], e.g. long-short-term memory (LSTM) which introduces the memory cell and forget gate into classical RNN network [38]. Furthermore, the state-of- the-art LSTM variants such as bidirectional LSTM (Bi- LSTM) [39], vanilla LSTM (V-LSTM) [40], stacked LSTM [41], convolutional LSTM (c-LSTM) [42] and convolutional neural network LSTM (CNN) [43] have shown more promising time series predictions [6] because of their capability of capturing rich information from complex time series data. In this research, we propose a deep learning blood glucose prediction model based on LSTM variants for improved prediction and clinical accuracy.\nto outperform\nFigure 1 Artificial Pancreas System\nThe remainder of this article is structured as follows. The related works are presented in Section 2, variations of LSTM are introduced in Section 3, dataset and training process are in Section 4, evaluation methods in Section 5, comments on results in Section 6, and conclusions in Section 7.\n2. Blood glucose prediction research\nBlood glucose prediction from physiological, data-driven and hybrid approach [10]. The physiological approach relies on expert knowledge on insulin and glucose metabolism focusing on simulation models [11, 12, 13]. The main challenge of physiological models is the lack of generalization capability and need support from data for higher prediction performance. Data-driven approaches are mainly based on machine learning methods such as fuzzy logic and rule-based models [14], multi-modal approaches [15, 16] autoregressive models [17, 18], support vector machine [19] and artificial neural networks models [20]. The hybrid approach includes physiological models such as glucose digestion and absorption, insulin absorptions, exercise, and other events. Those physiological models pre-process related data and the results are used in a data-driven model. [21, 22, 23]. Although there are existing studies on blood glucose prediction, longer-term accuracy remains the main challenge for blood glucose prediction studies [4]. Prediction horizon (PH) has been used in the vast majority of the studies for evaluation processes. Existing studies show an increase in the PH leads to a deterioration in the accuracy of a given model [4]. However, PH is important to be considered because patients\u2019 needs in deciding meals, physical activity, and other events happen over time. Therefore, both accuracy\nresearch\nrange\nresults\nfor\n3. LSTM variants based model for long- term blood glucose forecasting\n3.1 LSTM Long short-term memory(LSTM) is a special kind of recurrent neural network architecture(RNN). It is widely used on problems based on time series data such as speech recognition, handwriting recognition, prediction in healthcare pathways, etc. Unlike ordinary RNN, LSTM is specialized at manipulating Long-Term dependencies because it employs the \u201cremember\u201d mechanism through a series of gates. This feature fits the scenario of the glucose prediction problem because the observation window could be quite long which makes other machine learning methods difficult to\nthe accuracy of\nPage 3258\nhandle. The equations for the forward pass of the LSTM unit are as follows.\noutputs a sequence of vectors that will be used as the input of the subsequent LSTM layer.\nwhere the subscript t indexes the time step, the operator\u2218denotes the element-wise product.", "mimetype": "text/plain", "start_char_idx": 3979, "end_char_idx": 8449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea768f27-d3a3-41f5-9356-26a754f22c1a": {"__data__": {"id_": "ea768f27-d3a3-41f5-9356-26a754f22c1a", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "408dae0cd06bd88e4163f047edd9f950d5b5881da23a470e3633d8e6b48ca860", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58d3d8b7-fad9-4ffe-9973-e328f735883a", "node_type": "1", "metadata": {}, "hash": "64e4754c16548adf89b4b90f17b202193a92fa2a463624b82455b6428f3d2d97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, both accuracy\nresearch\nrange\nresults\nfor\n3. LSTM variants based model for long- term blood glucose forecasting\n3.1 LSTM Long short-term memory(LSTM) is a special kind of recurrent neural network architecture(RNN). It is widely used on problems based on time series data such as speech recognition, handwriting recognition, prediction in healthcare pathways, etc. Unlike ordinary RNN, LSTM is specialized at manipulating Long-Term dependencies because it employs the \u201cremember\u201d mechanism through a series of gates. This feature fits the scenario of the glucose prediction problem because the observation window could be quite long which makes other machine learning methods difficult to\nthe accuracy of\nPage 3258\nhandle. The equations for the forward pass of the LSTM unit are as follows.\noutputs a sequence of vectors that will be used as the input of the subsequent LSTM layer.\nwhere the subscript t indexes the time step, the operator\u2218denotes the element-wise product.\n\ud835\udc65\ud835\udc61: input vector to the LSTM unit \ud835\udc53\ud835\udc61: forget gate's activation vector \ud835\udc56\ud835\udc61: input/update gate's activation vector \ud835\udc5c\ud835\udc61: output gate's activation vector \u210e\ud835\udc61: hidden state vector also known as output vector of the LSTM unit \ud835\udc50\ud835\udc61: cell state vector W, U: weight matrices for input vectors and hidden vectors b: bias vector parameters\nFigure 3 Stacked LSTM\n3.3 CNN-LSTM\nDue to the intensity of glucose data, we employ CNN in order to better represent the latent features in the glucose series, combined with LSTM we have CNN-LSTM. CNN-LSTM is the combination of CNN layers and LSTM layers in order to take both advantages of CNN and LSTM. It is first designed for spatial inputs prediction problems like image sequence and video sequence prediction, recently it also has been applied in general time series prediction problems and acquired promising results. The architecture of CNN-LSTM as illustrated in Figure 4 includes Convolutional Neural Network(CNN) layers on feature extraction, a follow-up Max Pooling Layer for summarizing the most activated presence of a feature, then a pile of LSTM layers to handle the sequence processing and finally a Fully- Connected Layer before the output.\nThe architecture of the vanilla LSTM for glucose prediction is illustrated in Figure 2, a sequence of glucose values are input into the RNN-LSTM network and the target value is predicted at the end of the sequence.\nFigure 2 Vanilla LSTM\n3.2 Stacked LSTM\nThe success of deep neural networks attributes to its application of multiple layers. Each layer solves part of the task and altogether the complex network increases the representation power. We can also apply the same strategy on LSTM by adding more layers to make it deeper. The outcome of this idea is the so-called stacked-LSTM. As the name implies, it is an extension of the vanilla LSTM network by stacking a sequence of LSTM layers. Figure 2 gives the architecture of stacked-LSTM, which LSTM layers(vertically). In operation, each LSTM layer\nFigure 4 CNN-LSTM\n3.4 ConvLSTM\nhas\nseveral\nPage 3259\nConvLSTM is another way to leverage both CNN and LSTM. Instead of putting a CNN layer before the LSTM layer, ConvLSTM modifies the internal computation logic and convolution operation in the LSTM cell. In running time, ConvLSTM first read the input with the convolutional part and feed the output into each LSTM unit. The most obvious part exchanged in ConvLSTM is replace matrix operations that multiplication. So we have, e.g. the forget gate becomes '*' denotes convolution. Other formulas listed in Section 3.1 are updated in the same way. The structure of ConvLSTM presents as follows:\n5 Evaluation\nWe evaluate the results from two perspectives. One is the statistical evaluation which we use root-mean- squared-error(RMSE) to evaluate the prediction ability of the model. Another is the clinical accuracy evaluation which we employ the Clarke error analysis.\nconvolution\n, where\nRMSE is the square root of the average squared difference between predicted values and the actual values. In general, the lower this value means a better average prediction performance. The RMSE formation can be illustrated as follows:\nwhere \ud835\udc66\ud835\udc58is the actual value and \ud835\udc66\u0302\ud835\udc58is the predicted one. Although RMSE is widely used in the evaluation of time series prediction, it takes each value equally and only looks at the value difference. However, in medical practice like glucose management, different values may have significant difference in clinician outcome. Thus we introduce Clarke Error Grid analysis which pays more attention to the medical significance and amplify the prediction errors that could lead to risk treatments.", "mimetype": "text/plain", "start_char_idx": 7468, "end_char_idx": 12102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58d3d8b7-fad9-4ffe-9973-e328f735883a": {"__data__": {"id_": "58d3d8b7-fad9-4ffe-9973-e328f735883a", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea768f27-d3a3-41f5-9356-26a754f22c1a", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "034806a77fa92a77852393f06256f741951a18a6dacc5ed7bb50f964a89c9f67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4f61817-9940-491f-a132-58400ea21b8e", "node_type": "1", "metadata": {}, "hash": "0f9e555e8c25f37324fcc9694468c109d2cf37b2637569dff38c667177e882fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One is the statistical evaluation which we use root-mean- squared-error(RMSE) to evaluate the prediction ability of the model. Another is the clinical accuracy evaluation which we employ the Clarke error analysis.\nconvolution\n, where\nRMSE is the square root of the average squared difference between predicted values and the actual values. In general, the lower this value means a better average prediction performance. The RMSE formation can be illustrated as follows:\nwhere \ud835\udc66\ud835\udc58is the actual value and \ud835\udc66\u0302\ud835\udc58is the predicted one. Although RMSE is widely used in the evaluation of time series prediction, it takes each value equally and only looks at the value difference. However, in medical practice like glucose management, different values may have significant difference in clinician outcome. Thus we introduce Clarke Error Grid analysis which pays more attention to the medical significance and amplify the prediction errors that could lead to risk treatments. As shown in figure 6, 7 and 8 of the error grids, the horizontal axis represents true blood glucose and the vertical axis represents predicted blood glucose by the model. Specifically, breaks down the true-predicted blood glucose value scatter plots into five clinical meaningful regions. The regions signify the degree of risk posed by the incorrect prediction.\nFigure 5 ConvLSTM\nThe structure looks similar to vanilla LSTM except that the cells are replaced by ConvLSTM cells and the sequence is chunked and shaped to 3 dimensions to meet the needs of convolution calculation.\n4. Data and training\nThe data in this paper comes from donated CGM data of the OpenAPS project [7,8,9]. The glucose values are recorded every 5 minutes. We selected twenty persons' datasets from youth and adult age groups respectively which have the most integrity in one period. The overall number of time points is 878k, of which 1/3 are reserved as test data. Regarding the setting of the training and target window, we consulted clinicians about their practice in evaluating patients' glucose history and make a decision as follows: The training sliding window sizes are 60 minutes and 120 minutes respectively as they demonstrated better performance than other window sizes. So the input represented with time steps is 12 and 24. The prediction horizons are 30 minutes, 45 minutes and 60 minutes respectively. That gives the output length is 6, 9 and 12. There are limited missing values in the datasets, we filled them with linear interpolation The hardware for the training task includes 1 x\nSection A. Predicted blood glucose value is within 20% of the actual blood glucose values. This means the prediction error has no effect on clinical action therefore these points are also called clinical accurate ones which are appropriate to lead to the interventions. \u25cf Section B. Predicted value is beyond 20% but would not lead to inappropriate treatment. The prediction error has little or no effect on clinical outcomes. \u25cf Section C. The points in this area indicate the prediction errors might indicate an unnecessary treatment. \u25cf Section D. The points in this area means the prediction errors will lead to a dangerous failure of detecting hypoglycemia and hyperglycemia\nNVIDIA Tesla V100 and 4vCPU 26G memory, e.g.\nThe software that we used includes Pandas for data wrangling and Keras-LSTM library for training, the batch size is 128, the number of epochs is 100.\nPage 3260\nSection E. The points in this area means the prediction error could lead to dangerous consequences and it will and confuse hyperglycemia. Of all the 5 sections, the percentage of A+B states how the prediction algorithm performs in a clinical acceptance way, while we should also be aware of C, D, and E which symbolizes the errors that may lead to miss judgment in treatments. More percentage in A means less errors thus more clinically accurate predictions.\nIn addition to prediction accuracy, we also evaluated the clinical accuracy using the Clarke Error analysis to understand the clinical value of the proposed methods. Table 2 shows the score of Clarke zone A and zone B of CNN, v-LSTM, bi-LSTM, s-LSTM, CNN-LSTM, convLSTM and SVR over 30 minutes, 45 minutes and 60 minutes prediction horizon.\nTable 2 The Clinical Accuracy from Clarke Error Analysis\nMethod\\ PH(min)\n45 min\n30 min\n60 min\n6. Results 6.1 Prediction accuracy in RMSE We compared the performance among LSTM variants based models and support vector regression (SVR) method as the baseline. Table 1 shows the blood glucose prediction accuracy of CNN, v-LSTM, CNN-LSTM convLSTM and SVR over the prediction horizon of 30, 45 and 60 minutes.", "mimetype": "text/plain", "start_char_idx": 11140, "end_char_idx": 15769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4f61817-9940-491f-a132-58400ea21b8e": {"__data__": {"id_": "f4f61817-9940-491f-a132-58400ea21b8e", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58d3d8b7-fad9-4ffe-9973-e328f735883a", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "54f7ac83369b82a987221788ab592fe8168536428b716bc80821d390b7fc15d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f4605df-f31e-449e-92d1-93d1274a727e", "node_type": "1", "metadata": {}, "hash": "74ff2095a68aa903eadff81d272dc5021a3c7acadd0466cc030b6504a300a377", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More percentage in A means less errors thus more clinically accurate predictions.\nIn addition to prediction accuracy, we also evaluated the clinical accuracy using the Clarke Error analysis to understand the clinical value of the proposed methods. Table 2 shows the score of Clarke zone A and zone B of CNN, v-LSTM, bi-LSTM, s-LSTM, CNN-LSTM, convLSTM and SVR over 30 minutes, 45 minutes and 60 minutes prediction horizon.\nTable 2 The Clinical Accuracy from Clarke Error Analysis\nMethod\\ PH(min)\n45 min\n30 min\n60 min\n6. Results 6.1 Prediction accuracy in RMSE We compared the performance among LSTM variants based models and support vector regression (SVR) method as the baseline. Table 1 shows the blood glucose prediction accuracy of CNN, v-LSTM, CNN-LSTM convLSTM and SVR over the prediction horizon of 30, 45 and 60 minutes. We notice that stacked-LSTM gives the best RMSE performance under the short-term(30 minutes) and mid-term(45 minutes) horizon. Vanilla- LSTM has the lowest score on long-term(60 minutes) horizon prediction. However, vanilla-LSTM only outperforms stacked-LSTM with 0.1 difference(19.01 vs 19.24) .If we consider the overall performance on all prediction horizons, stacked-LSTM achieves the best score. Besides, all the LSTM variant models outperform SVR in 30, 45 and 60 prediction horizons. 60 min\nBzon e\nAzone Bzon\nAzone Bzone\nAzo ne\ne\n0.304\n0.732\n0.652\nCNN\n0.124\n0.84 4\n0.230\n0.309\n0.182\n0.782\n0.650\n0.108\n0.87 1\nvanilla- LSTM\n0.88 7\n0.250\nstacked- LSTM\n0.700\n0.089\n0.784\n0.181\n0.700\n0.110\n0.214\n0.257\n0.86 1\nCNN- LSTM\n0.748\n0.713\n0.782\n0.102\n0.182\n0.245\nconvLST M\n0.86 8\n0.213\n0.645\n0.180\n0.703\n0.80 4\n0.107\nSVR\nWe can learn that all methods have a high clinical acceptance rate because the total score of zone A and B for each LSTM variant based model has an average above 0.95. When we look at the long term prediction accuracy, convLSTM gives the best performance with zone A score of 0.713 in 60 minutes prediction horizon. Stacked-LSTM shows the best performance in 30 and 45 minutes prediction horizons with Clarke zone A score of 0.887 and 0.784 respectively. Figure 6-8 illustrates points distribution of the best prediction on Clarke Grid Analysis on 60 minutes, 45 minutes, and 30 minutes prediction horizons respectively. It\u2019s clear that the majority of points spread in ZoneA+B which is good to lead the treatment. When we look at Zone C-D, compared with zone-C, points zone-D develops fast as the target horizon increases from 30 minutes to 60 minutes. It indicates that the failure of detecting hypoglycemia and hyperglycemia increases as we predict farther in the future. That is one potential orientation for optimization.\n21.04\u00b12.45\n18.08\u00b11.94\nCNN\n14.74\u00b11.06\n12.33\u00b11.15\nvanilla- LSTM\n19.01\u00b12.62\n15.86\u00b11.80\n19.24\u00b11.78\n15.81\u00b11.56\nstacked- LSTM\n11.96\u00b11.02\nCNN-LSTM\n19.80\u00b12.54\n13.05\u00b11.21\n16.72\u00b12.28\n19.60\u00b12.01\nconvLSTM\n12.20\u00b10.94\n15.82\u00b11.85\n13.28\u00b11.02\nSVR\n17.89\u00b11.34\n24.21\u00b12.96\n6.2 Clinical Accuracy\nPage 3261\n7. Conclusion\nIn this paper, we developed LSTM variants based blood glucose prediction models for improved prediction and clinical accuracy in long prediction horizon. The long-term modified LSTM can capture more dependencies due to deeper architecture and learn to remove background noise, outline important features and better captures both future and past context of the input sequence.", "mimetype": "text/plain", "start_char_idx": 14941, "end_char_idx": 18285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f4605df-f31e-449e-92d1-93d1274a727e": {"__data__": {"id_": "2f4605df-f31e-449e-92d1-93d1274a727e", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4f61817-9940-491f-a132-58400ea21b8e", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "751566c7bee7d80b12b9c794a6755b6673ee4d72e3f1a36db8396273b51e9667", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5a722ad-ad75-4912-bb0f-12a236644ffd", "node_type": "1", "metadata": {}, "hash": "a5fe0286a5774fce1f97bcf56b37298b3b258beb03d8f06547d93b81ed5fb3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusion\nIn this paper, we developed LSTM variants based blood glucose prediction models for improved prediction and clinical accuracy in long prediction horizon. The long-term modified LSTM can capture more dependencies due to deeper architecture and learn to remove background noise, outline important features and better captures both future and past context of the input sequence. We evaluate the prediction and clinical accuracy of the long term (above 30 minutes) of the proposed methods using 20 cases of real-life data from results were OpenAPS compared between LSTM variants and to those learning algorithms and widely used established algorithms applied to the real-time prediction of glucose using CGM data. Prediction Horizons (PH) of 30, 45, and 60 minutes were used. The proposed LSTM variant in based methods showed superior performance forecasting BG levels (RMSE and clinical accuracy) against existing methods. For several other works, it is difficult through direct comparison due to the availability of benchmark datasets. However, we may compare the results with widely used methods as benchmarks, such as SVR. The results show that our approach suggests superiority in their prediction accuracy over the 30, 45, and 60 minute time period than existing studies [43][46]. As far as we know, the proposed algorithm achieves a performance state-of-the-art accuracy with regard to RMSE and clinical accuracy.\ncommunity. Prediction\nFigure 6 Clarke Error of convLSTM on PH 60(min)\nto evaluate\nthe RMSE\nFigure 7 Clarke Error of Stacked LSTM on PH 45(min)\nThere are several limitations and future work for this research. First, the longest prediction horizon evaluated is 60 minutes and future work will further improve the proposed models for longer term prediction towards more than 4 hours. Second, future life events will be considered over the longer prediction horizons to improve the performance. A hybrid model combining the advantages of both physiological and LSTM based approach could be developed. Thirdly, there is timing effects that users of OpenAPS have DIY systems making insulin dosing adjustments and acting upon them so it will affect the prediction results. We will consider quantify these influences and make the prediction more accurate. Although some works suggest that ingested carbohydrate information, along with injected insulin information might be redundant [44, 45], we will in future incorporate more clinical information such as comorbidities and other information from Electronic Patient Record for detailed patients\nFigure 8 Clarke Error of Stacked LSTM on PH 30(min)\nPage 3262\nphenotyping and personalized prediction model development. Finally, we have demonstrated the application of deep learning based blood glucose prediction model in the real-life data but more data from both OpenAPS and patients under various closed-loop system could reflect a wider population.\n[15].Buckingham B, Chase HP, Dassau E, Cobry E, Clinton P, Gage V, et al. Prevention of Nocturnal Hypoglycemia. Diabetes Care. 2010;33: 1013\u20131017. pmid:20200307 [16]Wang Y, Wu X, Mo X. A novel adaptive-weighted- average framework for blood glucose prediction. Diabetes Technol Ther. 2013;15: 792\u2013801. pmid:23883406 [17].Lu Y, Gribok A V., Ward WK, Reifman J. The importance of different frequency bands in predicting subcutaneous glucose concentration in type 1 diabetic patients. IEEE Trans Biomed Eng. 1839\u20131846. pmid:20403780 [18]Novara C, Pour NM, Vincent T, Grassi G. A Nonlinear Blind Identification Approach to Modeling of Diabetic Patients. Proc 19th World Congr Int Fed Autom Control. 2015; 1\u20139. [19] Clarke, William L. \"The original Clarke error grid analysis (EGA).\" Diabetes technology & therapeutics 7.5 (2005): 776-779. [20].Fernandez de Canete J, Gonzalez-Perez S, Ramos-Diaz JC. Artificial neural networks for closed loop control of in silico and ad hoc type 1 diabetes. Comput Methods Programs Biomed. 2012;106: 55\u201366. pmid:22178070 [21]Balakrishnan NP, Rangaiah GP, Samavedham L. Personalized blood glucose models for exercise, meal and insulin interventions in type 1 diabetic children. 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE; 2012. pp. 1250\u20131253.", "mimetype": "text/plain", "start_char_idx": 17899, "end_char_idx": 22146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5a722ad-ad75-4912-bb0f-12a236644ffd": {"__data__": {"id_": "e5a722ad-ad75-4912-bb0f-12a236644ffd", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f4605df-f31e-449e-92d1-93d1274a727e", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "c34cbc9a4eac70c885ca0ae5355d563a9a3707c6903c198b2cea5cfa98a37afc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23b822d0-f03d-4d55-9707-a849a754097d", "node_type": "1", "metadata": {}, "hash": "4dc8a5bcc210cd34de0845c0f4f9a84712e1549e5440404d84d9cb9800dc0f41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proc 19th World Congr Int Fed Autom Control. 2015; 1\u20139. [19] Clarke, William L. \"The original Clarke error grid analysis (EGA).\" Diabetes technology & therapeutics 7.5 (2005): 776-779. [20].Fernandez de Canete J, Gonzalez-Perez S, Ramos-Diaz JC. Artificial neural networks for closed loop control of in silico and ad hoc type 1 diabetes. Comput Methods Programs Biomed. 2012;106: 55\u201366. pmid:22178070 [21]Balakrishnan NP, Rangaiah GP, Samavedham L. Personalized blood glucose models for exercise, meal and insulin interventions in type 1 diabetic children. 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE; 2012. pp. 1250\u20131253. 10.1109/EMBC.2012.6346164 [22] Estrada GC, Kirchsteiger H, Eric R. Innovative Approach for Online Prediction of Blood Glucose Profile in Type 1 Diabetes Patients. Am Control Conf (ACC), 2010. 2010; 2015\u20132020. [23] Zecchin C, Facchinetti A, Sparacino G, Cobelli C. Jump neural network for online short-time prediction of blood glucose from continuous monitoring sensors and meal information. Comput Methods Programs Biomed. Elsevier Ireland Ltd; 2014;113: 144\u2013152. pmid:24192453 [24] Li K, Daniels J, Liu C, Herrero-Vinas P, Georgiou P. Convolutional for glucose recurrent neural networks prediction. IEEE Journal of Biomedical and Health Informatics. 2019 Apr 1. [25] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for fast feature embedding,\u201d in Proceedings of the 22nd ACM International Conference on Multimedia, ser. MM \u201914. New York, NY, USA: ACM, 2014, pp. 675\u2013678. [26] G. Litjens, C. I. Sanchez, N. Timofeeva, M. Hermsen, I. Nagtegaal, I. Kovacs, C. H. van de Kaa, P. Bult, B. van Ginneken, and J. van der Laak, \u201cDeep learning as a tool for increased accuracy and efficiency of histopathological diagnosis,\u201d Scientific Reports, vol. 6, p. 26286, May 2016. [27] R. Miotto, F. Wang, S. Wang, X. Jiang, and J. T. Dudley, \u201cDeep learning for healthcare: review, opportunities and challenges,\u201d Briefings in Bioinformatics, vol. 19, no. 6, pp. 1236\u20131246, 05 2017. [28] T. Zhu, K. Li, P. Herrero, J. Chen, and P. Georgiou, \u201cA deep learning algorithm for personalized blood glucose prediction,\u201d in The 3rd International Workshop on Knowledge\n9. References\n2010;57:\n[1]Diabetes diabetes. https://www.diabetes.co.uk/type1-diabetes.html. Accessed 14 June 2019. [2] Blauw H, Keith-Hynes P, Koops R, DeVries JH. A review of safety and design requirements of the artificial pancreas. Annals of biomedical engineering. 2016 Nov 1;44(11):3158- 72. [3] Diabetes UK. Research spotlight \u2013 the artificial pancreas. https://www.diabetes.org.uk/research/research-round- up/research-spotlight/research-spotlight-the-artificial- pancreas. Accessed 14 June 2019. [4] Medtronic MiniMed Inc. Summary of Safety and Effectiveness Data MiniMed 670G System.; 2016. [5] Oviedo S, Vehi J, Calm R, Armengol J. A Review Of Personalized Blood Glucose Prediction Strategies For T1dm Patients. j numer method biomed eng. 2016; pmid:27644067 [6] Salehinejad H, Sankar S, Barfett J, Colak E, Valaee S. Recent advances in recurrent neural networks. arXiv preprint arXiv:1801.01078.", "mimetype": "text/plain", "start_char_idx": 21469, "end_char_idx": 24680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23b822d0-f03d-4d55-9707-a849a754097d": {"__data__": {"id_": "23b822d0-f03d-4d55-9707-a849a754097d", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5a722ad-ad75-4912-bb0f-12a236644ffd", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "571a7291e2c6a2a7cb465518cb20b5e843adeba75ad65daf3cb9850d795b7510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fe5bdb3-cb30-4d98-bf62-5be46d14ae7d", "node_type": "1", "metadata": {}, "hash": "5c17b888b1c7d331a516dd94b437f000a7ffd0db62ff6fac07284d4a48c13bd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Diabetes UK. Research spotlight \u2013 the artificial pancreas. https://www.diabetes.org.uk/research/research-round- up/research-spotlight/research-spotlight-the-artificial- pancreas. Accessed 14 June 2019. [4] Medtronic MiniMed Inc. Summary of Safety and Effectiveness Data MiniMed 670G System.; 2016. [5] Oviedo S, Vehi J, Calm R, Armengol J. A Review Of Personalized Blood Glucose Prediction Strategies For T1dm Patients. j numer method biomed eng. 2016; pmid:27644067 [6] Salehinejad H, Sankar S, Barfett J, Colak E, Valaee S. Recent advances in recurrent neural networks. arXiv preprint arXiv:1801.01078. 2017 Dec 29. [7] OpenAPS. Open APS. https://openaps.org/ Accessed February 28, 2018. [8] Litchman ML, Lewis D, Kelly LA, Gee PM. Twitter analysis of# OpenAPS DIY artificial pancreas technology use suggests improved A1C and quality of life. Journal of diabetes science and technology. 2019 Mar;13(2):164-70. [9] Lewis D. OpenAPS Outcomes. OpenAPS.org. 2018. [10] Contreras I, Oviedo S, Vettoretti M, Visentin R, Veh\u00ed J. Personalized blood glucose prediction: A hybrid approach using grammatical evolution and physiological models. PloS one. 2017 Nov 7;12(11):e0187754. [11] Man CD, Micheletto F, Lv D, Breton M, Kovatchev B, Cobelli C. The UVA/PADOVA Type 1 Diabetes Simulator: New Features. J Diabetes Sci Technol. 2014;8: 26\u201334. pmid:24876534 [12] Fernandez M, Villasana M, Streja D. Glucose dynamics in Type I diabetes: Insights from the classic and linear minimal models. Comput Biol Med. 611\u2013627. pmid:16867301 [13] Ghosh S. A differential evolution based approach for estimating minimal model parameters from IVGTT data. Comput Biol Med. 2014;46: 51\u201360. pmid:24529205 [14] Fong S, Mohammed S, Fiaidhi J, Kwoh CK. Using causality modeling and Fuzzy Lattice Reasoning algorithm for predicting blood glucose. Expert Syst Appl. 2013;40: 7354\u2013 7366.\nUK.\nType\n1\nInt\n2017. Available\nat:\n2007;37:\nPage 3263\nDiscovery in Healthcare Data, IJCAI-ECAI 2018, Stockholm, Sweden, July 2018. [29] Y. Bengio, \u201cDeep learning of representations: Looking forward,\u201d in Statistical Language and Speech Processing. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013, pp. 1\u2013 37. [30] J. Schmidhuber, \u201cDeep learning in neural networks: An overview,\u201d Neural Networks, vol. 61, pp. 85 \u2013 117, 2015. [31] K. Li, A. Javer, E. Keaveny, and A. Brown, \u201cRecurrent neural networks with interpretable cells predict and classify worm behaviour,\u201d in Workshop on Worm\u2019s Neural Information Processing (WNIP) in NIPS, CA, USA, 2017. [32] Q. Zhang, Y. N. Wu, and S.-C. Zhu, \u201cInterpretable convolutional neural networks,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2018, pp. 8827\u20138836. [33] Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning to align and translate. ICLR (2015). [34] Sutskever, I., Vinyals, O. & Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112 (2014). [35] Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Process. Mag. IEEE 29, 82\u201397 (2012).", "mimetype": "text/plain", "start_char_idx": 24072, "end_char_idx": 27233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fe5bdb3-cb30-4d98-bf62-5be46d14ae7d": {"__data__": {"id_": "3fe5bdb3-cb30-4d98-bf62-5be46d14ae7d", "embedding": null, "metadata": {"filename": "0321.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68ec127c-3368-45d8-ba12-1ab8d1b5da7f", "node_type": "4", "metadata": {"filename": "0321.pdf"}, "hash": "cdcbc52a5554521419f4ccf08bff26253a51aa5d83b5dbba4701be5a0d52fa58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23b822d0-f03d-4d55-9707-a849a754097d", "node_type": "1", "metadata": {"filename": "0321.pdf"}, "hash": "6eb0c89096ac4ebc17413e035fce15dcb221ec374598db05217bfc334c032423", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhu, \u201cInterpretable convolutional neural networks,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2018, pp. 8827\u20138836. [33] Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning to align and translate. ICLR (2015). [34] Sutskever, I., Vinyals, O. & Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112 (2014). [35] Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Process. Mag. IEEE 29, 82\u201397 (2012). [36] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning Long- Term Dependencies with Graident Descent is Difficult,\u201d Saudi Med J, vol. 33, pp. 3\u20138, 2012. [37] S. Park, S. Min, H.-S. Choi, and S. Yoon, \u201cDeep Recurrent Neural Network-Based Identification of Precursor microRNAs,\u201d Nips, no. Nips, 2017. [38] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997. [39] P. Su, X. Ding, Y. Zhang, F. Miao, and N. Zhao, \u201cLearning to Predict Blood Pressure with Deep Bidirectional LSTM Network,\u201d pp. 1\u201319, 2017.\n[40] Wu Y, Yuan M, Dong S, Lin L, Liu Y. Remaining useful life estimation of engineered systems using vanilla LSTM neural networks. Neurocomputing. 2018 Jan 31;275:167-79. [41] A. Graves, \u201cGenerating sequences with recurrent neural networks,\u201d arXiv preprint arXiv:1308.0850, 2013. [42] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, \u201cAdieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5200\u20135204. [43] Sainath TN, Vinyals O, Senior A, Sak H. Convolutional, long short-term memory, fully connected deep neural networks. International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015 Apr 19 (pp. 4580-4584). IEEE. [44] E. I. Georga, V. C. Protopappas, D. Ardigo, M. Marina, I. Zavaroni, D. Polyzos, and D. I. Fotiadis, \u201cMultivariate prediction of subcutaneous glucose concentration in type 1 diabetes patients based on support vector regression,\u201d IEEE Journal of Biomedical and Health Informatics, vol. 17, no. 1, pp. 71\u201381, Jan 2013. [45] Zecchin, C Online glucose prediction in type 1 diabetes by neural network models. Univ. Degli Stud. Di Padova. Sch. Inf. Eng. Sect. Bioeng. XXVI Ser.; 2014. [46] Lu, Y, Gribok, AV, Ward, WK, Reifman, J. The importance of different frequency bands in predicting subcutaneous glucose concentration in type 1 diabetic patients. IEEE Trans Biomed Eng. 2010; 57( 8): 1839\u2013 1846. [47] K. Plis, R. Bunescu, C. Marling, J. Shubrook, and F. Schwartz, \u201cA machine learning approach to predicting blood glucose levels for diabetes management,\u201d in Modern Artificial Intelligence for Health Analytics Papers from the AAAI-14.\nIn2015\nIEEE", "mimetype": "text/plain", "start_char_idx": 26615, "end_char_idx": 29583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4854438e-115d-43ce-8e87-5c2e18dedd86": {"__data__": {"id_": "4854438e-115d-43ce-8e87-5c2e18dedd86", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "178ab083-1bba-4829-8bac-8e9f33656769", "node_type": "1", "metadata": {}, "hash": "2c390739b85e2b8f780e0b05b91ccb026bfdb25183cf98181677b3c8e5192477", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proceeding Paper An Open Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting \u2020\nGissel Velarde *\n, Pedro Bra\u00f1ez, Alejandro Bueno, Rodrigo Heredia and Mateo Lopez-Ledezma\nIndependent Researchers, Cochabamba 06651, Bolivia; pedrobran8@gmail.com (P.B.); alebuenoaz@gmail.com (A.B.); rodrigoh1205@gmail.com (R.H.); lopezmateo97@yahoo.com (M.L.-L.) * Correspondence: gv@urubo.org \u2020 Presented at the 8th International Conference on Time Series and Forecasting, Gran Canaria, Spain,\n27\u201330 June 2022.\n(cid:1)(cid:2)(cid:3)(cid:1)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)\nCitation: Velarde, G.; Bra\u00f1ez , P.;\nBueno, A.; Heredia, R;\nAbstract: This paper introduces an open source and reproducible implementation of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks for time series forecasting. We evaluated LSTM and GRU networks because of their performance reported in related work. We describe our method and its results on two datasets. The \ufb01rst dataset is the S&P BSE BANKEX, composed of stock time series (closing prices) of ten \ufb01nancial institutions. The second dataset, called Activities, comprises ten synthetic time series resembling weekly activities with \ufb01ve days of high activity and two days of low activity. We report Root Mean Squared Error (RMSE) between actual and predicted values, as well as Directional Accuracy (DA). We show that a single time series from a dataset can be used to adequately train the networks if the sequences in the dataset contain patterns that repeat, even with certain variation, and are properly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU networks signi\ufb01cantly outperform a baseline on the Activities dataset. The baseline simply repeats the last available value. On the stock market dataset, the networks perform just as the baseline, possibly due to the nature of these series. We release the datasets used as well as the implementation with all experiments performed to enable future comparisons and to make our research reproducible.\nLopez-Ledezma, M. An Open Source\nand Reproducible Implementation of\nLSTM and GRU Networks for Time\nSeries Forecasting. Eng. Proc. 2022,\nKeywords: forecasting; time series; open source; reproducibility\n18, 30. https://doi.org/10.3390/\nengproc2022018030\nAcademic Editors: Ignacio Rojas,\n1. Introduction\nHector Pomares, Olga Valenzuela,\nFernando Rojas and Luis Javier\nHerrera\nPublished: 22 June 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional af\ufb01l-\niations.\nArti\ufb01cial Neural Networks (ANNs) and particularly Recurrent Neural Networks (RNNs) gained attention in time series forecasting due to their capacity to model depen- dencies over time [1]. With our proposed method, we show that RNNs can be successfully trained with a single time series to deliver forecasts for unseen time series in a dataset containing patterns that repeat, even with certain variation; therefore, once a network is properly trained, it can be used to forecast other series in the dataset if adequately prepared. LSTM [2] and GRU [3] are two related deep learning architectures from the RNN family. LSTM consists of a memory cell that regulates its \ufb02ow of information thanks to its non-linear gating units, known as the input, forget and output gates, and activation functions [4]. GRU architecture consists of reset and update gates and activation functions. Both architectures are known to perform equally well on sequence modeling problems, yet GRU was found to train faster than LSTM on music and speech applications [5].\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under\nthe terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\nEmpirical studies on \ufb01nancial time series data reported that LSTM outperformed Autoregressive Integrated Moving Average (ARIMA) [6].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "178ab083-1bba-4829-8bac-8e9f33656769": {"__data__": {"id_": "178ab083-1bba-4829-8bac-8e9f33656769", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4854438e-115d-43ce-8e87-5c2e18dedd86", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "b6fb0e6309c236137925353e433e3e866023f6213fc8b34cd736c7afd13492a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef6162f4-0792-4c66-830f-be31e5eceef7", "node_type": "1", "metadata": {}, "hash": "54b7ee5d926b06205437408ce8f8a2dd4111839bd1cbb19c90c3325dd5efc020", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LSTM [2] and GRU [3] are two related deep learning architectures from the RNN family. LSTM consists of a memory cell that regulates its \ufb02ow of information thanks to its non-linear gating units, known as the input, forget and output gates, and activation functions [4]. GRU architecture consists of reset and update gates and activation functions. Both architectures are known to perform equally well on sequence modeling problems, yet GRU was found to train faster than LSTM on music and speech applications [5].\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under\nthe terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\nEmpirical studies on \ufb01nancial time series data reported that LSTM outperformed Autoregressive Integrated Moving Average (ARIMA) [6]. ARIMA [7] is a traditional forecasting method that integrates autoregression with moving average processes. In [6], LSTM and ARIMA were evaluated on RMSE between actual and predicted values on \ufb01nancial data. The authors suggested that the superiority of LSTM over ARIMA was thanks to gradient descent optimization [6]. A systematic study compared different ANNs\n4.0/).\nEng. Proc. 2022, 18, 30. https://doi.org/10.3390/engproc2022018030\nhttps://www.mdpi.com/journal/engproc\nEng. Proc. 2022, 18, 30\narchitectures for stock market forecasting [8]. More speci\ufb01cally, the authors evaluated architectures of the types LSTM, GRU, Convolutional Neural Networks (CNN) and Extreme Learning Machines (ELM). In their experiments, two-layered LSTM and two-layered GRU networks delivered low RMSE.\nIn this study, we evaluate LSTM and GRU architectures because of their performance reported in related work for time series forecasting [6,8]. Our method is described in Section 2. In Sections 2.1\u20132.3, we review principles of Recurrent Neural Networks (RNN) of the type LSTM and GRU. In Section 2.4, we explain our data preparation, followed by the networks\u2019 architecture, training (Section 2.5) and evaluation (Section 2.6). The evaluation was performed on two datasets. In Section 3.1, we describe the S&P BSE-BANKEX or simply BANKEX dataset, which was originally described in [8] and consists of stock time series (closing prices). In Section 3.2, we describe the Activities dataset, a dataset composed of synthetic time series resembling weekly activities with \ufb01ve days of high activity and two days of low activity. The experiments are presented in Section 3. Finally, we state our conclusions in Section 5 and present possible directions for future work. We release the datasets used as well as the implementation with all experiments performed to enable future comparisons and make our research reproducible.\n2. Method\nThe general overview of the method is described as follows. The method inputs time series of values over time and outputs predictions. Every time series in the dataset is normalized. Then, the number of test samples is de\ufb01ned to create the training and testing sets. One time series from the train set is selected and prepared to train an LSTM and a GRU, independently. Once the networks are trained, the test set is used to evaluate RMSE and DA between actual and predicted values for each network. The series are transformed back to unnormalized values for visual inspection. We describe every step in detail. Next, in Sections 2.1\u20132.3, we review principles of RNNs of the type LSTM and GRU, following the presentation as in [5].\n2.1. Recurrent Neural Networks\nANNs are trained to approximate a function and learn the networks\u2019 parameters that best approximate that function. RNNs are a special type of ANNs developed to handle sequences. An RNN updates its recurrent hidden state ht for a sequence x = (x1, x2,. . ., xT) by:\nht =\n(cid:26)\n0, \u03c6(ht\u22121, xt),\nt = 0 otherwise,\nwhere \u03c6 is a nonlinear function. The output of an RNN maybe of variable length y = (y1,y2,. . .,yT).", "mimetype": "text/plain", "start_char_idx": 3160, "end_char_idx": 7137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef6162f4-0792-4c66-830f-be31e5eceef7": {"__data__": {"id_": "ef6162f4-0792-4c66-830f-be31e5eceef7", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "178ab083-1bba-4829-8bac-8e9f33656769", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "5ad4e0eccf33563f7e43a4baa97950d82768734c87eeae154666cb5c1a8e8ee9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055ccaa7-9790-4892-978c-888419b70a49", "node_type": "1", "metadata": {}, "hash": "d537ededacd527b9f7febc64c0941c66cb3881b2cb9f291d6a3e6ba23066ee40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The series are transformed back to unnormalized values for visual inspection. We describe every step in detail. Next, in Sections 2.1\u20132.3, we review principles of RNNs of the type LSTM and GRU, following the presentation as in [5].\n2.1. Recurrent Neural Networks\nANNs are trained to approximate a function and learn the networks\u2019 parameters that best approximate that function. RNNs are a special type of ANNs developed to handle sequences. An RNN updates its recurrent hidden state ht for a sequence x = (x1, x2,. . ., xT) by:\nht =\n(cid:26)\n0, \u03c6(ht\u22121, xt),\nt = 0 otherwise,\nwhere \u03c6 is a nonlinear function. The output of an RNN maybe of variable length y = (y1,y2,. . .,yT).\nThe update of ht is computed by:\nht = g(Wxt + Uht\u22121),\nwhere W and U are weights\u2019 matrices and g is a smooth and bounded activation function such as a logistic sigmoid, or simply called sigmoid function f(x) = \u03c3 = 1 1+e\u2212x, or a hyperbolic tangent function f(x) = tanh(x) = ex\u2212e\u2212x ex+e\u2212x.\nGiven a state ht, an RNN outputs a probability distribution for the next element in a\nsequence. The sequence probability is represented as:\np(x1, x2,. . ., xT) = p(x1)p(x2 | x1) . . . p(xT | x1, x2,. . ., xT\u22121).\nThe last element is a so-called end-of-sequence value. The conditional probability\ndistribution is given by:\np(xt | x1, x2,. . . xt\u22121) = g(ht),\n2 of 9\n(1)\n(2)\n(3)\n(4)\nEng. Proc. 2022, 18, 30\n3 of 9\nwhere ht is the recurrent hidden state of the RNN as in expression (1). Updating the network\u2019s weights involves several matrix computations, such that back-propagating errors lead to vanishing or exploding weights, making training unfeasible. LSTM was proposed in 1997 to solve this problem by enforcing constant error \ufb02ow thanks to gating units [2]. GRU is a closely related network proposed in 2014 [3]. Next, we review LSTM and GRU networks. See Figure 1 for illustration.\nFigure 1. LSTM (left) and GRU (right). c represents the memory cell and \u02dcc the new memory cell of the LSTM. h represents the activation and \u02dch the new activation of the GRU. Based on [5].\n2.2. Long Short-Term Memory\nThe LSTM unit decides whether to keep content memory thanks to its gates. If a sequence feature is detected to be relevant, the LSTM unit keeps track of it over time, modeling dependencies over long-distance [5].\nIn Expressions (6)\u2013(8) and (10), W and U represent weights matrices and V represents a diagonal matrix. W, U and V need to be learned by the algorithm during training. The subscripts i, o and f correspond to input, output and forget gates, respectively. For every j-th LSTM unit, there is a memory cell cj\nt at time t, which activation hj\nt is computed as:\nt = oj hj\nttanh(cj t)\n(5)\nwhere oj The forget gate f j gate ij\nt is the output gate responsible for modulating the amount of memory in the cell. t modulates the amount of memory content to be forgotten and the input t modulates the amount of new memory to be added to the memory cell, such that:\noj t = \u03c3(Woxt + Uoht\u22121 + Voct)j,\n(6)\nf j t = \u03c3(Wf xt + Ufht\u22121 + Vfct\u22121)j, ij t = \u03c3(Wixt + Uiht\u22121 + Vict\u22121)j.\n(7)\n(8)\nwhere \u03c3 is a sigmoid function. The memory cell cj content \u02dccj\nt partially forgets and adds new memory\nt by:\ncj t = f j tcj\nt\u22121 + ij t\u02dccj t,\n(9)\nwhere:\n\u02dccj t = tanh(Wcxt + Ucht\u22121)j.\n(10)\n2.3.", "mimetype": "text/plain", "start_char_idx": 6462, "end_char_idx": 9700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "055ccaa7-9790-4892-978c-888419b70a49": {"__data__": {"id_": "055ccaa7-9790-4892-978c-888419b70a49", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef6162f4-0792-4c66-830f-be31e5eceef7", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "809d60401e53b95a52912e9be215f7459d21cc5a3824b78ab02714666dec0f71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41bf9b4f-31e6-4c0b-9777-9262ea2c447e", "node_type": "1", "metadata": {}, "hash": "71de7f9f7fd8f4400a90a7b1d646d22cc62ccc7677b510ed2627e81334124043", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "t modulates the amount of memory content to be forgotten and the input t modulates the amount of new memory to be added to the memory cell, such that:\noj t = \u03c3(Woxt + Uoht\u22121 + Voct)j,\n(6)\nf j t = \u03c3(Wf xt + Ufht\u22121 + Vfct\u22121)j, ij t = \u03c3(Wixt + Uiht\u22121 + Vict\u22121)j.\n(7)\n(8)\nwhere \u03c3 is a sigmoid function. The memory cell cj content \u02dccj\nt partially forgets and adds new memory\nt by:\ncj t = f j tcj\nt\u22121 + ij t\u02dccj t,\n(9)\nwhere:\n\u02dccj t = tanh(Wcxt + Ucht\u22121)j.\n(10)\n2.3. Gated Recurrent Unit\nThe main difference between LSTM and GRU is that GRU does not have a separate\nmemory cell, such that the activation hj\nt is obtained by the following expression:\nt = (1 \u2212 zj hj\nt)hj\nt\u22121 + zj\nt\n\u02dchj t.\n(11)\nEng. Proc. 2022, 18, 30\nThe update gate zj t\u22121 and candidate activation \u02dchj t. In Expressions (12)\u2013(14), W and U represent weights matrices that need to be learned during training. Moreover, the subscripts z and r correspond to update and reset gates, respectively. The update gate zj t are obtained by the following expressions:\nt decides the amount of update content given by the previous hj\nt and reset gate rj\nzj t = \u03c3(Wzxt + Uzht\u22121)j, rj t = \u03c3(Wrxt + Urht\u22121)j, where \u03c3 is a sigmoid function. The candidate activation \u02dchj\nt is obtained by:\n\u02dchj t = tanh(Wxt + rt (cid:12) (Uht\u22121))j.\nwhere (cid:12) denotes element-wise multiplication.\n2.4. Data Preparation\nEvery time series or sequence in the dataset is normalized as follows. Let v be a\nsequence v = (v1,v2,. . .,vQ) of Q samples that can be normalized between 0 and 1:\nx = v(cid:48) =\nv \u2212 vmin vmax \u2212 vmin\n.\nWe de\ufb01ne the number of samples in the test set as tests. The number of samples N for training is obtained by N = Q \u2212 tests \u2212 w. Then, a sequence x is selected arbitrarily and prepared to train each network as follows. We de\ufb01ne a window of size w and a number of steps ahead f, where f < w < N < Q, such that:\nX =\n\uf8ee\n\uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\nx1 x2 x3 . . . xQ\u2212(w\u22121+f)\nx2 x3 x4 . . . xQ\u2212(w\u22122+f)\n. . . . . . . . . . . . . . .\nxw xw+1 xw+2 . . . xQ\u2212f\n\uf8f9\n\uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\n,\nX becomes a Q \u2212 (w \u2212 1 + f) by w matrix, and:\nY =\n\uf8ee\n\uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\nxw+1 xw+2 xw+3 . . . xQ\u2212(f\u22121)\nxw+2 xw+3 xw+4 . . . xQ\u2212(f\u22122)\n. . . . . . . . . . . . . . .\nxw+f xw+2+f xw+3+f . . . xQ\n\uf8f9\n\uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nbecomes a Q \u2212 (w \u2212 1 + f) by f matrix containing the targets. The \ufb01rst N rows of X and Y are used for training. The remaining Q \u2212 N elements are used for testing. The settings for our experiments are described in Section 3, after we introduce the characteristics of the dataset used.\n2.5. Networks\u2019 Architecture and Training\nWe tested two RNNs.", "mimetype": "text/plain", "start_char_idx": 9242, "end_char_idx": 11776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41bf9b4f-31e6-4c0b-9777-9262ea2c447e": {"__data__": {"id_": "41bf9b4f-31e6-4c0b-9777-9262ea2c447e", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "055ccaa7-9790-4892-978c-888419b70a49", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "27f20436580c44cbbe642d1c627bf45e5be3c48ed0bae79aff22e77fd0e99906", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "002cf1b1-9963-4b4e-9dc3-d013939dc585", "node_type": "1", "metadata": {}, "hash": "b64cbcdb197b4f9888db8121335e2231b90964fd74e26b1c77d355e0faeee199", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "xQ\u2212(f\u22121)\nxw+2 xw+3 xw+4 . . . xQ\u2212(f\u22122)\n. . . . . . . . . . . . . . .\nxw+f xw+2+f xw+3+f . . . xQ\n\uf8f9\n\uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nbecomes a Q \u2212 (w \u2212 1 + f) by f matrix containing the targets. The \ufb01rst N rows of X and Y are used for training. The remaining Q \u2212 N elements are used for testing. The settings for our experiments are described in Section 3, after we introduce the characteristics of the dataset used.\n2.5. Networks\u2019 Architecture and Training\nWe tested two RNNs. One with LSTM memory cells and one with GRU memory cells.\nIn both cases, we use the following architecture and training:\n\u2022\nA layer with 128 units, A dense layer with size equal to the number of steps ahead for prediction,\nwith recurrent sigmoid activations and tanh activation functions as explained in Sections 2.2 and 2.3. The networks are trained for 200 epochs with Adam optimizer [9]. The number of epochs and architecture were set empirically. We minimize Mean Squared Error (MSE) loss between the targets and the predicted values, see Expression (17). The\n4 of 9\n(12)\n(13)\n(14)\n(15)\n(16)\nEng. Proc. 2022, 18, 30\nnetworks are trained using a single time series prepared as described in Section 2.4. The data partition is explained in Section 3.3.\n2.6. Evaluation\nWe use Mean Squared Error (MSE) to train the networks:\nMSE = n\u22121\nn \u2211 t=1\n(xt \u2212 yt)2,\nwhere n is the number of samples, xt and yt are actual and predicted values at time t. Moreover, we use Root Mean Squared Error (RMSE) for evaluation between algorithms:\nRMSE =\n\u221a\nMSE.\nBoth metrics, MSE and RMSE, are used to measure the difference between actual and predicted values, and therefore, smaller results are preferred [10]. We also use Directional Accuracy (DA):\nDA =\n100 n\nn \u2211 t=1\ndt,\nwhere:\ndt =\n(cid:26)1 0\n(xt \u2212 xt\u22121)(yt \u2212 yt\u22121) \u2265 0 otherwise.\nsuch that xt and yt are the actual and predicted values at time t, respectively, and n is the sample size. DA is used to measure the capacity of a model to predict direction as well as prediction accuracy. Thus, higher values of DA are preferred [10].\n3. Experiments\nIn this section, we report experiments performed with both datasets.\n3.1. The S&P BSE BANKEX Dataset\nThis dataset was originally described in [8]; however, our query retrieved a different number of samples as in [8]. We assume it must have changed since it was originally retrieved. We collected the time series on 20 January 2022, using Yahoo! Finance\u2019s API [11] for the time frame between 12 July 2005, and 3 November 2017, see Table 1. Most time series had 3035 samples, and some time series had 3032 samples; therefore, we stored each time series\u2019s last 3032 samples. Figure 2 presents the time series of BANKEX without and with normalization.\nTable 1. Entities in the S&P BSE-BANKEX Dataset.\nNumber\nEntity\nSymbol\n1 2 3 4 5 6 7 8 9 10\nAxis Bank Bank of Baroda Federal Bank HDFC Bank ICICI Bank Indus Ind Bank Kotak Mahindra PNB SBI Yes Bank\nAXISBANK.BO BANKBARODA.BO FEDERALBNK.BO HDFCBANK.BO ICICIBANK.BO INDUSINDBK.BO KOTAKBANK.BO PNB.BO SBIN.BO YESBANK.BO\n5 of 9\n(17)\n(18)\n(19)\nEng. Proc. 2022, 18, 30\n(a)\n(b)\nFigure 2. (a) Time series in the BANKEX dataset without normalization. Closing Price in Indian Rupee (INR). Daily samples retrieved between 12 July 2005 and 3 November 2017 using Yahoo! Finance\u2019s API [11]. All time series with 3032 samples.", "mimetype": "text/plain", "start_char_idx": 11321, "end_char_idx": 14614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "002cf1b1-9963-4b4e-9dc3-d013939dc585": {"__data__": {"id_": "002cf1b1-9963-4b4e-9dc3-d013939dc585", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41bf9b4f-31e6-4c0b-9777-9262ea2c447e", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "dd55922cbe5809ad327695b840c9814a30cd60a56371e8c1cbac17d1a8778ebb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c54336c-70bd-4cd5-bff0-48e4773d2172", "node_type": "1", "metadata": {}, "hash": "e63b5053d0eacf6255350a079d00aa2706b4cd8a481572dfea4febdb75d96766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proc. 2022, 18, 30\n(a)\n(b)\nFigure 2. (a) Time series in the BANKEX dataset without normalization. Closing Price in Indian Rupee (INR). Daily samples retrieved between 12 July 2005 and 3 November 2017 using Yahoo! Finance\u2019s API [11]. All time series with 3032 samples. (b) Same time series as in (a), but with normalization; closing price normalized between 0 and 1. The numbers from 1 to 10 correspond to the numbers (\ufb01rst column) for each series in Table 1.\n3.2. The Activities Dataset\nThe Activities dataset is a synthetic dataset created resembling weekly activities with \ufb01ve days of high activity and two days of low activity. The dataset has ten time series with 3584 samples per series. Initially, a pattern of \ufb01ve ones followed by two zeros was repeated to obtain a length of 3584 samples. The series was added a slope of 0.0001. The original series was circularly rotated for the remaining series in the dataset, to which noise was added, and each sequence was arbitrarily scaled, so that the peak-to-peak amplitude of each series was different, see Figure 3.\n6 of 9\nEng. Proc. 2022, 18, 30\n7 of 9\nFigure 3. Time series in the Activities dataset without normalization, \ufb01rst 100 samples.\n3.3. Datasets Preparation and Partition\nFollowing Section 2.4, every time series was normalized between 0 and 1. We used a window of size w = 60 days. We tested for f = 1 and f = 20 steps ahead. We used the last 251 samples of each time series for testing. We selected arbitrarily the \ufb01rst time series of each dataset for training our LSTM and GRU networks.\n3.4. Results\nThe results are presented in Tables 2\u20135. Close-to-zero RMSE and close-to-one DA are preferred. On the Activities dataset, two-tailed Mann\u2013Whitney tests show that for 1-step ahead forecasts, RMSE achieved by any RNN is signi\ufb01cantly lower than that de- livered by the baseline (LSTM & Baseline: U = 19,n = 10,10, p < 0.05. GRU & Baseline: U = 0,n = 10,10, p < 0.05). In addition, GRU delivers signi\ufb01cantly lower RMSE than LSTM (U = 91,n = 10,10, p < 0.05). In terms of DA, both RNN perform equally well and signi\ufb01cantly outperform the baseline. For 20-step ahead forecasts, again both RNNs achieve signi\ufb01cantly lower RMSE than the baseline (LSTM & Baseline: U = 0,n = 10,10, p < 0.05. GRU & Baseline: U = 0,n = 10,10, p < 0.05). This time, LSTM achieves lower RMSE than GRU (U = 10,n = 10,10, p < 0.05) and higher DA (U = 81,n = 10,10, p < 0.05).\nTable 2. One-step ahead forecast on Activities dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.\nRMSE\nDA\nLSTM\nGRU\nBaseline\nLSTM\nGRU\nBaseline\nMean SD\n0.2949 0.0941\n0.1268 0.0425\n0.3730 0.0534\n0.6360 0.0455\n0.6236 0.0377\n0.4212 0.0403\nTable 3. Twenty-step ahead forecast on Activities dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.\nRMSE\nDA\nLSTM\nGRU\nBaseline\nLSTM\nGRU\nBaseline\nMean SD\n0.1267 0.0435\n0.2048 0.0683\n0.4551 0.0678\n0.6419 0.0331\n0.6261 0.0255\n0.4805 0.0413\nTable 4. One-step ahead forecast on BANKEX dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.", "mimetype": "text/plain", "start_char_idx": 14347, "end_char_idx": 17322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c54336c-70bd-4cd5-bff0-48e4773d2172": {"__data__": {"id_": "5c54336c-70bd-4cd5-bff0-48e4773d2172", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "002cf1b1-9963-4b4e-9dc3-d013939dc585", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "4b4182f91c0495f745da207374c34c3541f4d0bbd74efc2ce2cb4d42f92828d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "381082d5-7242-4bf8-88f0-c4c5bbea87e3", "node_type": "1", "metadata": {}, "hash": "c7d8a61cac2ffc2ea69b19a76b2b2ceb40fc9d42141eac52364c41138e29ba5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Twenty-step ahead forecast on Activities dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.\nRMSE\nDA\nLSTM\nGRU\nBaseline\nLSTM\nGRU\nBaseline\nMean SD\n0.1267 0.0435\n0.2048 0.0683\n0.4551 0.0678\n0.6419 0.0331\n0.6261 0.0255\n0.4805 0.0413\nTable 4. One-step ahead forecast on BANKEX dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.\nRMSE\nDA\nLSTM\nGRU\nBaseline\nLSTM\nGRU\nBaseline\nMean SD\n0.0163 0.0052\n0.0163 0.0056\n0.0161 0.0056\n0.4884 0.0398\n0.4860 0.0385\n0.4880 0.0432\nEng. Proc. 2022, 18, 30\n8 of 9\nTable 5. Twenty-step ahead forecast on BANKEX dataset. RMSE: columns 2 to 4. DA: columns 5 to 7.\nRMSE\nDA\nLSTM\nGRU\nBaseline\nLSTM\nGRU\nBaseline\nMean SD\n0.0543 0.0093\n0.0501 0.0064\n0.0427 0.0113\n0.5004 0.0071\n0.5004 0.0087\n0.4969 0.0076\nOn the BANKEX dataset, two-tailed Mann\u2013Whitney tests show that for 1-step ahead forecasts there is no difference among approaches considering RMSE (LSTM & Baseline: U = 51,n = 10,10, p > 0.05. GRU & Baseline: U = 55,n = 10,10, p > 0.05. LSTM & GRU: U = 49,n = 10,10, p > 0.05). Similar results are found for 20-step ahead forecasts (LSTM & Baseline: U = 76,n = 10,10, p > 0.05. GRU & Baseline: U = 67,n = 10,10, p > 0.05. LSTM & GRU: U = 66,n = 10,10, p > 0.05). DA results are consistent with those obtained for RMSE. Figure 4a,b show examples of 20-step ahead forecasts and Figure 5 presents an example of 1-step ahead forecasts. Visual inspection helps understand the results.\n(a)\n(b)\nFigure 4. Examples of 20-step ahead forecast. (a) Activities dataset. (b) BANKEX dataset.\nFigure 5. Example of 1-step ahead forecast. Actual and predicted closing price over the \ufb01rst 100 days of the test set Yes Bank. Closing Price in Indian Rupee (INR).\n4. Discussion\nThe motivation for developing a reproducible and open-source framework for time series forecasting relates to our experience in trying to reproduce previous work [6,8]. We found it challenging to \ufb01nd implementations that are simple to understand and replicate. In addition, datasets are not available. We discontinued comparisons with [8], since the dataset\nEng. Proc. 2022, 18, 30\nwe collected was slightly different, and we were unsure if the reported results referred to normalized values or not. If the algorithms are described but the implementations are not available, a dataset is necessary to compare forecasting performance between two algorithms, such that a statistical test can help determine if one algorithm is signi\ufb01cantly more accurate than the other [12] (pp. 580\u2013581).\n5. Conclusions\nWe proposed a method for time series forecasting based on LSTM and GRU and showed that these networks can be successfully trained with a single time series to deliver forecasts for unseen time series in a dataset containing patterns that repeat, even with certain variation. Once a network is properly trained, it can be used to forecast other series in the dataset if adequately prepared. We tried and varied several hyperparameters.", "mimetype": "text/plain", "start_char_idx": 17001, "end_char_idx": 19916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "381082d5-7242-4bf8-88f0-c4c5bbea87e3": {"__data__": {"id_": "381082d5-7242-4bf8-88f0-c4c5bbea87e3", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c54336c-70bd-4cd5-bff0-48e4773d2172", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "af6d959a3fc4f7a92d80dc2e4c8cde11f85108f592598adfcfcb2e639a438507", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f719ef32-c5f6-492b-b6b4-8de6a89f8d77", "node_type": "1", "metadata": {}, "hash": "1d045124e1d004e827bcba2be339951c00d01d789906fceee7e7251c8f1f4ad5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We discontinued comparisons with [8], since the dataset\nEng. Proc. 2022, 18, 30\nwe collected was slightly different, and we were unsure if the reported results referred to normalized values or not. If the algorithms are described but the implementations are not available, a dataset is necessary to compare forecasting performance between two algorithms, such that a statistical test can help determine if one algorithm is signi\ufb01cantly more accurate than the other [12] (pp. 580\u2013581).\n5. Conclusions\nWe proposed a method for time series forecasting based on LSTM and GRU and showed that these networks can be successfully trained with a single time series to deliver forecasts for unseen time series in a dataset containing patterns that repeat, even with certain variation. Once a network is properly trained, it can be used to forecast other series in the dataset if adequately prepared. We tried and varied several hyperparameters. On sequences, such as those resembling weekly activities that repeat with certain variation, we found an appropriate setting; however, we failed to \ufb01nd an architecture that would outperform a baseline on stock market data; therefore, we assume that we either failed at optimizing the hyperparameters of the networks, the approach is unsuitable for this application, or we would need extra information that is not re\ufb02ected in stock market series alone. For future work, we plan to benchmark different forecasting methods against the method presented here. In particular, we want to evaluate statistical methods as well as other machine learning methods that have demonstrated strong performance on forecasting tasks [13]. We release our code as well as the dataset used in this study to allow this research to be reproducible.\nAuthor Contributions: P.B., A.B., R.H. and M.L.-L. designed, implemented, and trained the networks, G.V. advised on the project, optimized the networks and code, and wrote the paper. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nData Availability Statement: The code and datasets to reproduce this research are available at: https: //github.com/Alebuenoaz/LSTM-and-GRU-Time-Series-Forecasting (accessed on 22 May 2022).\nAcknowledgments: We would like to thank the anonymous reviewers for their valuable observations.\nCon\ufb02icts of Interest: The authors declare no con\ufb02ict of interest.\nReferences\n1.\nRumelhart, D.E.; Hinton, G.E.; Williams, R.J. Learning representations by back-propagating errors. Nature 1986, 323, 533\u2013536. [CrossRef]\n2. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735\u20131780. [CrossRef] [PubMed] 3. Cho, K.; Van Merri\u00ebnboer, B.; Bahdanau, D.; Bengio, Y. On the properties of neural machine translation: Encoder-decoder approaches. arXiv 2014, arXiv:1409.1259. Greff, K.; Srivastava, R.K.; Koutn\u00edk, J.; Steunebrink, B.R.; Schmidhuber, J. LSTM: A search space odyssey. IEEE Trans. Neural Netw. Learn. Syst. 2016, 28, 2222\u20132232. [CrossRef] [PubMed] Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv 2014, arXiv:1412.3555. Siami-Namini, S.; Tavakoli, N.; Namin, A.S. A comparison of ARIMA and LSTM in forecasting time series. In Proceedings of the 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), Orlando, FL, USA, 17\u201320 December 2018; pp. 1394\u20131401. Box, G.; Jenkins, G. Time Series Analysis: Forecasting and Control; Holden-Day: San Francisco, CA, USA, 1970. Balaji, A.J.; Ram, D.H.; Nair, B.B. Applicability of deep learning models for stock price forecasting an empirical study on BANKEX data. Procedia Comput. Sci. 2018, 143, 947\u2013953. [CrossRef] Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization.", "mimetype": "text/plain", "start_char_idx": 18982, "end_char_idx": 22806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f719ef32-c5f6-492b-b6b4-8de6a89f8d77": {"__data__": {"id_": "f719ef32-c5f6-492b-b6b4-8de6a89f8d77", "embedding": null, "metadata": {"filename": "engproc-18-00030.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e76227a0-1079-4add-9293-ebba4cae6936", "node_type": "4", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "c4cdae52543619a5b7a226380d07edf692028e141a2f3f75802cd144ae92ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "381082d5-7242-4bf8-88f0-c4c5bbea87e3", "node_type": "1", "metadata": {"filename": "engproc-18-00030.pdf"}, "hash": "51346e9f823cf5779e75f773dcaf13de3d0165f0a3a420aab09c36e1b3fc2d5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Siami-Namini, S.; Tavakoli, N.; Namin, A.S. A comparison of ARIMA and LSTM in forecasting time series. In Proceedings of the 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), Orlando, FL, USA, 17\u201320 December 2018; pp. 1394\u20131401. Box, G.; Jenkins, G. Time Series Analysis: Forecasting and Control; Holden-Day: San Francisco, CA, USA, 1970. Balaji, A.J.; Ram, D.H.; Nair, B.B. Applicability of deep learning models for stock price forecasting an empirical study on BANKEX data. Procedia Comput. Sci. 2018, 143, 947\u2013953. [CrossRef] Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980.\n4.\n5.\n6.\n7. 8.\n9. 10. Wang, J.J.; Wang, J.Z.; Zhang, Z.G.; Guo, S.P. Stock index forecasting based on a hybrid model. Omega 2012, 40, 758\u2013766.\n[CrossRef]\n11. Yahoo. Yahoo! Finance\u2019s API. 2022. Available online: https://pypi.org/project/y\ufb01nance/ (accessed on 20 January 2022). 12. Alpaydin, E. Introduction to Machine Learning; The MIT Press: Cambridge, MA, USA; London, UK, 2014. 13. Makridakis, S.; Spiliotis, E.; Assimakopoulos, V. M5 accuracy competition: Results, \ufb01ndings, and conclusions. Int. J. Forecast.\n2022. [CrossRef]\n9 of 9", "mimetype": "text/plain", "start_char_idx": 22177, "end_char_idx": 23371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a2722fe-d6c2-4cd5-9b74-d730cea19600": {"__data__": {"id_": "8a2722fe-d6c2-4cd5-9b74-d730cea19600", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5856ed92-9a46-4e6b-a8af-b9523f877ec8", "node_type": "1", "metadata": {}, "hash": "b5dc86bf140bdc852dcfcba21b111881e48226588c12a6d827c2399c418376f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IET Software\nResearch Article\nSoftware defect prediction via LSTM\nISSN 1751-8806 Received on 23rd May 2019 Revised 25th March 2020 Accepted on 6th April 2020 E-First on 15th May 2020 doi: 10.1049/iet-sen.2019.0149 www.ietdl.org\nJiehan Deng1, Lu Lu1 , Shaojian Qiu1 1School of Computer Science and Engineering, South China University of Technology, GuangZhou, People's Republic of China\nE-mail: lul@scut.edu.cn\nAbstract: Software quality plays an important role in the software lifecycle. Traditional software defect prediction approaches mainly focused on using hand-crafted features to detect defects. However, like human languages, programming languages contain rich semantic and structural information, and the cause of defective code is closely related to its context. Failing to catch this significant information, the performance of traditional approaches is far from satisfactory. In this study, the authors leveraged a long short-term memory (LSTM) network to automatically learn the semantic and contextual features from the source code. Specifically, they first extract the program's Abstract Syntax Trees (ASTs), which is made up of AST nodes, and then evaluate what and how much information they can preserve for several node types. They traverse the AST of each file and fed them into the LSTM network to automatically the semantic and contextual features of the program, which is then used to determine whether the file is defective. Experimental results on several opensource projects showed that the proposed LSTM method is superior to the state-of-the-art methods.\n1\u2003Introduction\nModern software is becoming more and more powerful, thus its scale and complexity continue to increase, which brought a great threat to the quality and reliability of software. To ensure software quality, companies have to employ quality assurance team to find defects in the software, which is a labour-intensive and costly work. However, to find as many bugs as possible, software testing often requires a large amount of time to perform various test cases, thus, with finite budgets and tight schedule, it is impractical to run the test for the entire project. Therefore, software defect prediction (SDP) has been proposed not only to reduce the cost and time for software testing, but also help the assurance team to locate the defective code more easily. And software defect prediction has attracted many researchers in recent years [1\u20134].\nSDP is a process of building a defect prediction model using the historical data and then predict where the new code is defective. Previous studies on SDP mainly focussed on using either machine learning or deep learning techniques to build an effective defect prediction model. The former approach used handcrafted features like Halstead features [5], McCabe features [6], CK features [7], and build classifiers using machine learning techniques such as logistic regression (LR) or support vector machine, while the latter used deep learning techniques to construct powerful neural networks and made use of the program's Abstract Syntax Trees (ASTs) to build prediction models. And many studies [8\u201311] have proven the success usage of AST.\nstatistical characteristics of the program, assuming that the buggy code has distinguishable features when compared to the clean ones. However, the handcrafted features of clean and buggy code can resemble, which will make it hard for classifiers to distinguish the difference.\nHandcrafted\nfeatures mainly\nfocus on\nthe\nFor example, Fig. 1a shows a non-buggy code snippet with two while loops. We simply switch the operation on the variable \u2018i\u2019 of the two while loops and now the file, shown in Fig. 1b is buggy as it will encounter an infinite loop.\nHowever, the length of code, the number of operands and operators or even its complexity remains unchanged, which is hard to tell whether it is defective through those statistical features. And our observation on real-world programs does show that there are a number of files whose handcrafted features are very identical before and after the bugs are fixed. So in this paper, we focus out point on the program's semantic and contextual features.\nLike human languages, programming languages have to follow certain syntax rules, which lead to rich semantic and contextual information that can be extracted from the program's ASTs. AST is the representation [12] of source code, and contains the information of how the code is organised and interacts with each other. Again, in Fig. 1, we can simply know that the cause of the infinite loop is not only the operation on variable \u2018i\u2019 is \u2018\u2013\u2019, but also \u2018i < 10\u2019 is the condition of the while loop and the result of \u2018\u2013\u2019 operation on \u2018i\u2019 will always meet the condition of the for a loop.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5856ed92-9a46-4e6b-a8af-b9523f877ec8": {"__data__": {"id_": "5856ed92-9a46-4e6b-a8af-b9523f877ec8", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a2722fe-d6c2-4cd5-9b74-d730cea19600", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "c168b0dc00f99ce768034b7bbc3c4724480f72fd41f968a0fa3c2ab16a319215", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bfa74d1-b907-44bf-8f95-c3f406701893", "node_type": "1", "metadata": {}, "hash": "2fad2502ca0c516ca3b64d308b8a0d278c74250cc64b7c4f96a9805d3bdf4bbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And our observation on real-world programs does show that there are a number of files whose handcrafted features are very identical before and after the bugs are fixed. So in this paper, we focus out point on the program's semantic and contextual features.\nLike human languages, programming languages have to follow certain syntax rules, which lead to rich semantic and contextual information that can be extracted from the program's ASTs. AST is the representation [12] of source code, and contains the information of how the code is organised and interacts with each other. Again, in Fig. 1, we can simply know that the cause of the infinite loop is not only the operation on variable \u2018i\u2019 is \u2018\u2013\u2019, but also \u2018i < 10\u2019 is the condition of the while loop and the result of \u2018\u2013\u2019 operation on \u2018i\u2019 will always meet the condition of the for a loop. A simple form of the two files\u2019 AST is shown in Fig. 2. The difference between the two ASTs is obvious \u2013 the operation on variable \u2018i\u2019, which shows where and what causes the different behaviour of the two code snippets. Such slight difference that leads to huge variance results is what handcrafted features failed to capture while it can be easily reflected by AST. So leveraging deep learning methods to mine this hidden information inside AST can lead to more accurate prediction results.\nFig. 1\u2004 Two highly identical code snippets (a) non-buggy code, (b) buggy code due caused by switching operators\nRecurrent neural networks (RNNs) [13] have been wildly applied in natural language process (NLP) like language modelling [14], machine translation [15], speech recognition [16], and video captioning [17]. RNN performs sequential processing by modelling units in sequence, and it can \u2018memorise\u2019 the information of previous computations. Since defective code is closely related to its context, thus RNNs are more suitable for capturing semantic and contextual features than other networks, such as CNN, which is\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n443\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nFig. 2\u2004 Difference of the two ASTs (a) AST of the non-buggy code, (b) AST of the buggy code\nLSTM approach in Section 3. Experimental setup and result analysis are presented in Section 4. Finally, we conclude our work in Section 5.\n2\u2003Related work\nSDP has become one of the most popular research fields in software engineering [18]. Aiming to predict defect-prone software modules, it can help software quality assurance team to better allocate limited testing resources. Traditional approaches that only use handcraft features may fail to capture the information that is hidden inside the program's AST, so many studies now focus on using deep learning techniques to mine valuable information from AST and use them to more accurate prediction models.\nFig. 3\u2004 Difference between the unit of RNN and LSTM\nmore capable of detecting local features. We also employed the word embedding technique, which is also used in NLP tasks, to map the AST tokens into numerical vectors and help RNN to better capture the semantic and contextual features.\n2.1 Deep learning in software defect prediction\nRecent years, some researchers [8, 19] have suggested that only consider handcrafted features it is not enough to perform SDP accurately. To expand the features available in SDP and improve prediction performance, they tried to use deep-learning methods (e.g. deep belief network, DBN [20] and convolutional neural network, CNN [19]) to mine the rich semantic and structural features hidden inside the source code. The main idea of these methods is to extract semantic and structural features from the token vectors generated by programs\u2019 ASTs and feed them to the machine-learning classifier to obtain the SDP model. Their experiments show that DBN and CNN methods are superior to the traditional SDP methods that use only handcrafted features in the SDP tasks.\nTo make good use of the semantic and contextual information inside the program's ASTs, this paper proposed a deep learning base SDP framework named defect prediction via LSTM (DP- LSTM), which can capture semantic and contextual features and use them to improve the performance of defect prediction.", "mimetype": "text/plain", "start_char_idx": 3923, "end_char_idx": 8471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bfa74d1-b907-44bf-8f95-c3f406701893": {"__data__": {"id_": "0bfa74d1-b907-44bf-8f95-c3f406701893", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5856ed92-9a46-4e6b-a8af-b9523f877ec8", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "9dbb151a5ec8dad557ca42f71a229925100249f13f5a78c6ff26d7795e8c030b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebb48b40-3514-4324-891c-025a784addee", "node_type": "1", "metadata": {}, "hash": "6cd52ab52485c3df25b7f69d2edd26fc391d19d1f2ada80f8df63670bb09320c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To expand the features available in SDP and improve prediction performance, they tried to use deep-learning methods (e.g. deep belief network, DBN [20] and convolutional neural network, CNN [19]) to mine the rich semantic and structural features hidden inside the source code. The main idea of these methods is to extract semantic and structural features from the token vectors generated by programs\u2019 ASTs and feed them to the machine-learning classifier to obtain the SDP model. Their experiments show that DBN and CNN methods are superior to the traditional SDP methods that use only handcrafted features in the SDP tasks.\nTo make good use of the semantic and contextual information inside the program's ASTs, this paper proposed a deep learning base SDP framework named defect prediction via LSTM (DP- LSTM), which can capture semantic and contextual features and use them to improve the performance of defect prediction. Specifically, we first parse the source code files into ASTs and traverse the AST to form the sequence of AST nodes for each file. Then we evaluate how much information we should preserve for a single AST node and transform them into numerical vectors using word embedding. The sequence of the AST node is then fed into the RNN to generate semantic and contextual features which are input to a Softmax classifier to determine whether the file is defective.\n2.2 LSTM network\nTo assess the proposed DP-LSTM approach, we explored the\nLong short-term memory (LSTM) network is a variation between RNN, which is specialised for processing sequences input like text and videos. Unlike other neural networks, RNNs can use their internal state (memory) to learn the relationship between the pieces of sequence, which makes it possible for RNNs to capture contextual features of the sequence. And RNNs have been proved successful in many NLP tasks like language modelling, machine translation, speech recognition, and image captioning. However, standard RNNs may encounter vanishing gradient problem [21] when the input sequence is too long, which make it impossible for RNNs to learn the connection from the information that has a longer gap. Thus, LSTM networks [22] are introduced to overcome the long-term dependency problem. In this paper, we leverage bidirectional LSTM for semantic and contextual feature generation from the program's ASTs.\nfollowing two research questions:\n(i) RQ1: Can DP-LSTM yield better performance than traditional methods that only use handcraft features to make prediction? (ii) RQ2: Compared to the state-of-the-art deep learning-based approaches, how effective is DP-LSTM approach?\nThis paper makes the following contributions:\n(i) We propose a defect prediction framework based on RNN to mine semantic and contextual information from the program's ASTs. Experimental results on seven open-source projects from PROMISE dataset show that the performance of DP-LSTM is better compared to the state-of-the-art deep learning-based method. (ii) We traverse the program's ASTs to form the AST node sequence for each source code file and apply dictionary mapping and word embedding techniques to convert the sequence into the input of RNN to capture the semantic and contextual information.\nFig. 3 shows the difference between RNN and LSTM units. Compared to standard RNN, a cell state and several cell state control gates are added to the neurons of LSTM, making LSTMs have long-term memory across the entire sequence. For standard RNNs, let Ot denote the output of moment t, the loss function at the moment t is: Lt = (1/2)(Yt \u2212 Ot)2 and the total loss for the entire\nThe rest of the paper is summarised as follows. Section 2 introduces the related work on SDP. We elaborate on our DP-\n444\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nT Lt. Let Wx denotes the input weights. sequence is L = \u2211t = 0 According to the chain rule, the divertive of Wx with respect to the loss function of any moment t is:\nthey are used (e.g. we can use the variable \u2018max\u2019 to store the minimum value). Thus, we treat the AST node as the basic unit and we traverse the AST to get the sequence of AST nodes.", "mimetype": "text/plain", "start_char_idx": 7547, "end_char_idx": 12070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebb48b40-3514-4324-891c-025a784addee": {"__data__": {"id_": "ebb48b40-3514-4324-891c-025a784addee", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bfa74d1-b907-44bf-8f95-c3f406701893", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "d5313f400a02aaaa998a3d041be6866358b03b132f3af057c7b802d68c16a69f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cb55855-869d-4ba8-bf0e-8a72b410e8ca", "node_type": "1", "metadata": {}, "hash": "4aa1fe5c1ab1c570965fc1daaa4441e5c7901c251aac1c9e52eb106bf5ec9529", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nT Lt. Let Wx denotes the input weights. sequence is L = \u2211t = 0 According to the chain rule, the divertive of Wx with respect to the loss function of any moment t is:\nthey are used (e.g. we can use the variable \u2018max\u2019 to store the minimum value). Thus, we treat the AST node as the basic unit and we traverse the AST to get the sequence of AST nodes.\n3.3 AST node mapping and embedding\nt = \u2211 k = 0\nt\n\u2202Ot \u2202St \u220f\n\u2202Lt \u2202Wx\n\u2202Sk \u2202Wx\n\u2202Lt \u2202Ot\n\u2202Sj \u2202Sj \u2212 1\n(1)\nSince the AST node sequence cannot be directly fed into the neural network, we need to convert the AST node sequence into numerical vectors. In this step, we first build a unified dictionary mapping between nodes and integer values for the source and target project. Simply mapping the node type into an integer is applicable but, to some extent, may fail to capture important information from the AST node. Directly mapping the specific name of the node can capture every detail from the AST. However, the naming of variables, methods and classes is project specific and is closely related to the programmer's habit (e.g. the method that returns the maximum value can be named as \u2018max\u2019, \u2018maxVal\u2019 or \u2018maxValue\u2019). Such detailed information is redundant for defect prediction and may obfuscate the classifier. However, node information such as modifiers and variable types is vital to defect detection, because to some extent, they can lead to the defect of the code.\nj = k + 1\nwhere Sj denotes the state of the RNN unit at the moment j. If we use tanh for the activation function of the neurons, we have:\nSj = tanh(Wx[hj \u2212 1, xj] + WsSj \u2212 1 + b1)\nt = \u220f j = k + 1\nt \u220f j = k + 1\n(2)\n\u2202Sj \u2202Sj \u2212 1\n(tanhWs)\u2032\nSince the derivative value of tanh( \u22c5 ) lies between 0 and 1, and only when \u22c5 equals to 0, the divertive value is 1. However, during the training process, the divertive value of tanh( \u22c5 ) is usually smaller than 1 in and when t becomes larger, the product of (tanhWs)\u2032 tend to approach 0, which means \u2202Lt/\u2202Wx will\nt\n\u220fj = k + 1 no longer be effective when t becomes larger.\nFor example, Fig. 5 shows two highly identical code snippets. Except the names of variables and methods, the only critical difference between them is the modifiers of the filed \u2018sdf1\u2019 (private) and \u2018sdf2\u2019 (private static). However, the second code snippet class defect-prone \u2018SimpleDateFormat\u2019 is thread unsafe and the instances of class \u2018Sample2\u2019 share the same filed \u2018sdf2\u2019. If \u2018method2\u2019 of these instances are invoked in different threads concurrently, it is very likely to cause an exception. Such kind of defects is often hard to discover, especially in modern software modules where multi- thread programming is wildly used. Figs. 6 and 7 show the ASTs with plain node type and ASTs with the specific node name of the two code snippets (nodes that are different from the two code snippets are coloured as yellow, otherwise, are coloured as blue). Obviously, neither of the ASTs can accurately indicate what causes the defect in the second code snippet.\nIn LSTM, however, whether the information from previous cell state should be preserved is now controlled by the forget gate ft = \u03c3(Wf \u22c5 [ht \u2212 1, xt] + bf).", "mimetype": "text/plain", "start_char_idx": 11528, "end_char_idx": 14859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9cb55855-869d-4ba8-bf0e-8a72b410e8ca": {"__data__": {"id_": "9cb55855-869d-4ba8-bf0e-8a72b410e8ca", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebb48b40-3514-4324-891c-025a784addee", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "9cc48d7e27edbe4b1e15950d3a9fae32171e9f07cda0936d993cef0edeeaf96a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d519525-935e-4fda-9d5d-a4b097534a98", "node_type": "1", "metadata": {}, "hash": "975b3716f87f44011182ce0b59380d3056745c1dd5849fc7db50a5dd4adb0a1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the second code snippet class defect-prone \u2018SimpleDateFormat\u2019 is thread unsafe and the instances of class \u2018Sample2\u2019 share the same filed \u2018sdf2\u2019. If \u2018method2\u2019 of these instances are invoked in different threads concurrently, it is very likely to cause an exception. Such kind of defects is often hard to discover, especially in modern software modules where multi- thread programming is wildly used. Figs. 6 and 7 show the ASTs with plain node type and ASTs with the specific node name of the two code snippets (nodes that are different from the two code snippets are coloured as yellow, otherwise, are coloured as blue). Obviously, neither of the ASTs can accurately indicate what causes the defect in the second code snippet.\nIn LSTM, however, whether the information from previous cell state should be preserved is now controlled by the forget gate ft = \u03c3(Wf \u22c5 [ht \u2212 1, xt] + bf). By adding forget gate, the state of the neuron at the moment t now becomes:\nis\nhighly\nthe\nbecause\nSj = tanh(\u03c3(ft)St \u2212 1 + WsSj \u2212 1 + b1)\nt \u220f j = k + 1\nt = \u220f j = k + 1\n(3)\n\u2202Sj \u2202Sj \u2212 1\n(tanh(\u03c3(ft))\u2032\nFor tanh(\u03c3( \u22c5 ))\u2032, nearly most of its value is either 0 or 1, so it is possible to control whether the information from previous cell state should be preserved through the forget gate. The effectiveness of LSTMs largely depends on its parameters such as hidden dimension and batch size. Thus, tuning is a key to train a successful LSTM. We will discuss how to tune these parameters in Section 6.2.\nTo achieve the balance between two kinds of AST, we go through the node type we selected in the previous step, and Table 2 shows the information we preserved for certain node types (other node types will be mapped into an integer using their type name). Fig. 8 shows the AST with information that we preserved, and the cause of the defect can be found. After the dictionary is built, we then use the word embedding technique to represent each token as a high-dimensional vector to let the network learn the relation between nodes.\n3\u2003Methodology\nIn this section, we elaborate our proposed DP-LSTM framework which can automatically generate semantic and contextual features from AST and utilise them for defect prediction.\n3.1 Overall architecture\n3.4 Bidirectional LSTM building\nFig. 4 demonstrates the overall structure of our proposed SDP framework which automatically learns the semantic and contextual features from the source code. Specifically, the framework contains four steps: (i) source code parsing and sequence converting; (ii) AST node mapping and embedding; (iii) bidirectional LSTM building; (iv) defect prediction.\nIn this step, we employed RNN to better utilise the semantic and contextual information of the AST. Standard RNN takes sequences as input, and each step of the sequence denotes a certain moment. For a certain moment t, the output ot not only depends on the current input xt but is also influenced by the output from the previous moment t \u2212 1. The output of moment t can be formulated as the following equations:\n3.2 Source code parsing and sequence converting\nThe input of out framework is labelled Java files from the previous version and unlabelled Java files from the newer version. Previous studies [23\u201325] have proven that program's AST contains rich information which can be mined and used for defect prediction, so we adopt AST as a high level representation of the program's source code. In this step, we use Javalang [26] to parse the Java files into ASTs and each line in Java file can be represented as one or several nodes in the AST. There are numerous types of tree nodes inside the AST, but not all of them relate to the defect of the code, so we evaluated the node types and Table 1 shows the categories and types of nodes we selected. In NLP tasks like text classification [27], words are treated as basic units in sentences, and a sentence is a sequence of multiple words. However, in a programming language, it is impractical to treat words as basic units [28] because the functional meanings of variables, methods and classes are not determined by their literal meanings but how\nht = f(U \u00d7 xt + W \u00d7 ht \u2212 1 + b) ot = g(V \u00d7 ht + c)\nwhere U, V, and W denote the weights of the RNN, b and c denote the bias. f and g are the activation functions of the neurons.", "mimetype": "text/plain", "start_char_idx": 13968, "end_char_idx": 18261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d519525-935e-4fda-9d5d-a4b097534a98": {"__data__": {"id_": "2d519525-935e-4fda-9d5d-a4b097534a98", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cb55855-869d-4ba8-bf0e-8a72b410e8ca", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "93282f3d34ec6ef42818952b387b4ebb0615d27240b2256127b33c6aff699e4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06308865-04e0-41e4-9337-1eb5aba373a4", "node_type": "1", "metadata": {}, "hash": "d6c10e7e3a70a4498b949127c3a4e4c148d6ab73a531ccce1c5b19d2e657ad4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are numerous types of tree nodes inside the AST, but not all of them relate to the defect of the code, so we evaluated the node types and Table 1 shows the categories and types of nodes we selected. In NLP tasks like text classification [27], words are treated as basic units in sentences, and a sentence is a sequence of multiple words. However, in a programming language, it is impractical to treat words as basic units [28] because the functional meanings of variables, methods and classes are not determined by their literal meanings but how\nht = f(U \u00d7 xt + W \u00d7 ht \u2212 1 + b) ot = g(V \u00d7 ht + c)\nwhere U, V, and W denote the weights of the RNN, b and c denote the bias. f and g are the activation functions of the neurons. However, as the length of the sequence continues to grow, standard RNN may encounter vanishing gradient problem due to the backpropagation mechanism, in other words, standard RNN can hardly learn the relationship from the information that requires long-term dependencies. To handle the vanishing gradient problem, we employed LSTM, which is a variation of RNN. Compared to standard RNN, LSTM can keep long-term memory by adding cell state and several gates to the RNN neurons. The cell state carries the information from the previous moments and will flow through the entire LSTM chain, which is the key that LSTM can have long-\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n(4)\n445\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nFig. 4\u2004 Difference of the unit of RNN and LSTM\nTable 1\u2003AST nodes we selected Node category nodes of method invocations and instance creations declaration-related nodes\nshould be filtered from the previous moment, the output of forget gate can be formulated as the following equation:\nNode type MethodInvocation, SuperMethodInvocation, ClassCreator\nft = \u03c3(Wf \u22c5 [ht \u2212 1, xt] + bf)\nwhere \u03c3 denotes the activation function, Wf and bf denote the weights and bias of the forget gate, respectively. The input gate determines what information should be kept from the current moment, and its output can be formulated as the following equation:\nPackageDeclaration, InterfaceDeclaration, ClassDeclaration, ConstructorDeclaration, MethodDeclaration, VariableDeclarator, FormalParameter IfStatement, ForStatement, WhileStatement, DoStatement, AssertStatement, BreakStatement, ContinueStatement, ReturnStatement, ThrowStatement, TryStatement, SynchronizedStatement, SwitchStatement, BlockStatement, CatchClauseParameter, TryResource, CatchClause, SwitchStatementCase, ForControl, EnhancedForControl BasicType, MemberReference, ReferenceType, SuperMemberReference, StatementExpression,\ncontrol-flow-related nodes\nit = \u03c3(Wi \u22c5 [ht \u2212 1, xt] + bi)\nwhere \u03c3 denotes the activation function, Wi and bi denote the weights and bias of the input gate, respectively. With the information from forget gate and input gate, the cell state Ct \u2212 1 is updated through the following formula:\nCt = tanh(WC \u22c5 [ht \u2212 1, xt] + bC)\nother nodes\nCt = ft \u00d7 Ct \u2212 1 + i \u00d7 Ct\nCt is a candidate value that is going to be added into the cell state and Ct is the current updated cell state. Finally, the output gate decides what information should be outputted according to the previous output and current cell state\not = \u03c3(Wo \u22c5 [ht \u2212 1, xt + bo]) ht = ot \u00d7 tanh(Ct).\nAs stated above, the defective code is closely related to its previous and subsequent code segments, so we build a bidirectional LSTM to better handle the context dependencies.\n3.5 Defect prediction\nRelying on the bidirectional LSTM, we can extract the semantic and contextual features from the AST node sequence and use it for defect prediction. In this step, we choose LR as the final classifier. We train the classifier using the previous version of the program, and then perform defect prediction on the newer version.\nFig. 5\u2004 Another highly identical code snippet\nterm memory.", "mimetype": "text/plain", "start_char_idx": 17532, "end_char_idx": 21741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06308865-04e0-41e4-9337-1eb5aba373a4": {"__data__": {"id_": "06308865-04e0-41e4-9337-1eb5aba373a4", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d519525-935e-4fda-9d5d-a4b097534a98", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "7d996d7c82ddc32886d20c6bf0fc9cdd7e084d3627fcf865bbb27bb666c5ad71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26538059-2366-4518-9021-dad98651bf99", "node_type": "1", "metadata": {}, "hash": "83ccbd6c6382678774622923e9b13db952b3e0055261f411f95b08b117ea2fad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the output gate decides what information should be outputted according to the previous output and current cell state\not = \u03c3(Wo \u22c5 [ht \u2212 1, xt + bo]) ht = ot \u00d7 tanh(Ct).\nAs stated above, the defective code is closely related to its previous and subsequent code segments, so we build a bidirectional LSTM to better handle the context dependencies.\n3.5 Defect prediction\nRelying on the bidirectional LSTM, we can extract the semantic and contextual features from the AST node sequence and use it for defect prediction. In this step, we choose LR as the final classifier. We train the classifier using the previous version of the program, and then perform defect prediction on the newer version.\nFig. 5\u2004 Another highly identical code snippet\nterm memory. The gates of LSTM consists of a forget gate, an input gate and an output gate, which control how the information is kept in the cell states. The forget gate decides what information\n446\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n(5)\n(6)\n(7)\n(8)\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nTable 3\u2003Detailed information for the projects we selected Avg. Project Defect Rate, % 47.3\nAvg. Files\nVersion (train, predict) 2.5, 2.6\nDescription\n844\nXalan\nA library for transforming XML files Libraries for reading and writing files in Microsoft Office formats A framework for message-oriented middleware Programmer's text editor Jedit Lucene Ultra-fast Search Library\n64.2\n413\n2.5.1, 3.0\nPoi\nFig. 6\u2004 AST with only node type\n18.1\n918\n1.4, 1.6\nCamel\n4.0, 4.1 2.0, 2.2\n309 221\n25.0 53.7\nand Server A Java logging tool An XML parser\n149 447\n1.1, 1.2 1.2, 1.3\n76.5 15.7\nLog4j Xercers\nguarantee the generality of the evaluation results, experimental datasets consist of projects with different sizes and defect rates (in ten projects, the maximum number of files is 965, and the minimum number of files is 32. In addition, the minimum defect rate is 6.3% and the maximum defect rate is 98.8%).\nFig. 7\u2004 AST with specific node name\nTable 2\u2003Information we preserved for each node type Information preserved Node type modifiers, parameter types MethodDeclaration modifiers FieldDeclaration method name, parameters\u2019 name MethodInvocation member name MemberReference modifiers ClassDeclaration variable type VariableDeclaration lock object name SynchronisedStatement condition IfStatement loop condition WhileStatement loop condition ForStatement\nHowever, most of the datasets suffer from class imbalance, which is a common phenomenon is SDP [4, 29]. As shown in Table 2, the defect rate of most projects are neither too high nor too low and if the classifier is trained on a highly imbalanced dataset, the predictive model tends to support the majority class. And the ability to detect the minority class is weak. Many studies have proposed various approaches [4, 30, 31] to deal with class imbalance, but since it is not the key point of this paper, we adopt the standard random oversampling method \u2013 SMOTE [32\u201334] to handle the class imbalance problem.\n4.3 Evaluation metrics\nThere are many evaluation metrics [35] for the prediction model. Like other binary classification tasks, the result has four types: classifying an actual defective file as defective (true positive, TP); classifying an actual clean file as defective (false positive, FP); classifying an actual defective file as clean (false negative, FN); and classifying an actual clean file as clean (true negative, TN). We can construct a confusion matrix from these results and Table 4 demonstrates the confusion matrix.\nAccording to the confusion matrix, we can get the definition of\nPrecision and Recall.\nPrecision (also known as positive predictive value) is the fraction of actual positive instances among all instances that are predicted as positive:\nFig.", "mimetype": "text/plain", "start_char_idx": 20983, "end_char_idx": 25102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26538059-2366-4518-9021-dad98651bf99": {"__data__": {"id_": "26538059-2366-4518-9021-dad98651bf99", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06308865-04e0-41e4-9337-1eb5aba373a4", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "7a5e5cdfbf6d967785d60149524f96ea2c41663487bf6225e924478bfb824346", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed433f3-d937-46c5-9b65-152e70ee9222", "node_type": "1", "metadata": {}, "hash": "718de15501ae4dd32bfc8259dd8e892b844d047f0e67595fe99457c0a30a2895", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.3 Evaluation metrics\nThere are many evaluation metrics [35] for the prediction model. Like other binary classification tasks, the result has four types: classifying an actual defective file as defective (true positive, TP); classifying an actual clean file as defective (false positive, FP); classifying an actual defective file as clean (false negative, FN); and classifying an actual clean file as clean (true negative, TN). We can construct a confusion matrix from these results and Table 4 demonstrates the confusion matrix.\nAccording to the confusion matrix, we can get the definition of\nPrecision and Recall.\nPrecision (also known as positive predictive value) is the fraction of actual positive instances among all instances that are predicted as positive:\nFig. 8\u2004 AST with selected info\n4\u2003Experimental setup\nTP TP + FP\nthis section, we describe\nthe detailed settings for our\nIn experiments.\n(9)\nPrecision =\nRecall (also known as sensitivity) is a measurement of the integrity and defines the probability of the number of TP instances compared with the number of all positive instances\n4.1 Research questions\n(i) RQ1: Can DP-LSTM yield better performance than traditional methods that only use handcraft features to make predictions (ii) RQ2: Compared to the state-of-the-art deep learning-based approaches, how effective is DP-LSTM approach?\nTP TP + FN\n(10)\nRecall =\nIn practice, however, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. For example, if we demand a higher recall, we can predict all files as defective. Though recall will reach 100%, the precision will be very low. So F-measure is introduced to make a balance between precision and recall and it is the harmonic mean of the precision and recall:\n4.2 Experimental datasets\nTo verify the validity of DP-LSTM approach, we chose seven open-source projects as our evaluation datasets. All these projects come from PROMISE repository which is a public dataset, and is used wildly in SDP studies. In our experiments, we extracted semantic and contextual features from Java source code using LSTM and adopted the labels from the PROMISE repository. Table 3 shows the detailed information of selected projects, including project name, project version, number of instances, and average defect rate (the percentage of defective instances). To\n2 \u22c5 Precision \u22c5 Recall Precision + Recall\nF\u2212measure =\n(11)\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n447\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\nTable 4\u2003Confusion matrix\nActual negative FP TN\nActual positive TP FN\npredicted positive predicted negative\nTable 5\u2003F-measure comparison of different models LR Project 0.344 Xalan 0.472 Poi 0.126 Cammel 0.249 Jedit 0.475 Lucene 0.308 Log4j 0.136 Xerces 7/0/0 W/T/L 0.301 average\nDP-CNN 0.541 0.729 0.364 0.459 0.592 0.481 0.252 6/0/1 0.488\nDBN\u2009+\u2009 0.634 0.639 0.325 0.472 0.629 0.516 0.245 7/0/0 0.480\nCNN 0.460 0.710 0.341 0.436 0.587 0.434 0.254 7/0/0 0.460\nDBN 0.513 0.561 0.292 0.397 0.625 0.472 0.217 7/0/0 0.411\nDP-LSTM 0.655 0.735 0.376 0.445 0.643 0.527 0.268 \u2014 0.521\nperformance of 0.301 which is lower than other deep learning- based approaches.\nFor this reason, we adopt F-measure [36] as the evaluation metric of our model.", "mimetype": "text/plain", "start_char_idx": 24332, "end_char_idx": 27965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ed433f3-d937-46c5-9b65-152e70ee9222": {"__data__": {"id_": "3ed433f3-d937-46c5-9b65-152e70ee9222", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26538059-2366-4518-9021-dad98651bf99", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "8275dc9eb0963a983061021b857a7aa69d0c817fe51cee6cf03efb4272c3f913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60b087a0-10d3-458a-b540-d518291d16d4", "node_type": "1", "metadata": {}, "hash": "546a2ab519cf9fafb252f61ef4d790e9576053c4e2c2d2b03e4c3281fa223420", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this reason, we adopt F-measure [36] as the evaluation metric of our model. F-measure is the most commonly used metric in many SDP studies.\n5.2 RQ2: compared to the state-of-the-art deep learning- based approaches, how effective is DP-LSTM approach?\n4.4 Model comparison\nTo further verify the validity of our approach, we chose several deep learning-based methods for comparison including:\nAs Table 5 shows, the results of the approach wins six times compared to other models. Despite the performance of DP-LSTM is not always best, the average F-measure of DP-LSTM is 0.521 which outperforms DBN, CNN, DP-DBN, DP-CNN by 26.7, 13.2, 8.5, and 6.7%, respectively.\ni.\nLR: An simple handcrafted features.\nlogistic regression classifier based on\nii. DBN: Using deep-belief-network as features extractor to extract semantic features hidden on source code. Then training LR using those features. Following Wang et al.'s [8] parameter settings, we set 10 hidden layers and 100 nodes each layer. iii. DBN\u2009+\u2009: Combining the sematic features extracted by DBN\n6\u2003Discussion\n6.1 Why does DP-LSTM work?\nExperimental results from Section 5 show that our proposed DP- LSTM method which can make connections between sequence pieces performs better than other models. The possible reasons are summarised as follows:\nwith the handcraft features.\niv. CNN: Giving a separate number for each AST node, then feeding it into the CNN. Referring to [37], the batch size is set as 32, the training epoch is 15, the filter length is 5 and the number of the hidden units is set as 100.\n(i) Compared with traditional SDP methods, DP-LSTM uses the rich semantic and contextual information from the AST, which can be hardly features. Such information indicates how the codes are organised and interact with each other. Like many deep learning-based methods, our DP- LSTM can seize these important features which are then used to train the classifier. (ii) Compared with other deep learning-based models (e.g. CNN, DBN), DP-LSTM can longer dependencies. Since defective codes are closely related to its context, but if the context becomes further, models like CNN, which focus on local features, may fail to capture the connection of the codes and its context. Also, the bidirectional structure of LSTM ensures that both previous and subsequent code segments are taken into consideration. Although CNN can generate semantic and contextual features, we believe that features generated by DP- LSTM can better represent how the defect is caused.\nv. DP-CNN: Introducing handcrafted features base on CNN.\nrepresented by\nthe handcrafted\nFor fair comparations, we use LR as the classifier for all models. The implementation of LR is from sklearn.linear_model and we adopted the default parameters. Moreover, we follow the same approach to parse the source code into AST and convert to numerical vectors. To avoid randomness caused by oversampling, we repeated the experiment for at least ten times and recorded the average performance.\nthe\nrelationship of\nlearn\nthe\n5\u2003Results\nTable 5 lists the F-measure results of all models on the selected projects and the highest values are marked as bold. Take project Xalan as an example, the F-measure value for LR, DBN, CNN are 0.344, 0.513 and 0.460, while DP-DBN and DP-CNN, DP-LSTM can achieve 0.634, 0.541 and 0.655, respectively. From Table 5, we can find that DP-CNN and DP-DBN achieve better results since they combined deep learning generated features and handcrafted features, while DP-LSTM can achieve an even better result without combining the handcrafted features. The average result in the last row indicates that DP-LSTM outperforms other deep learning- based methods.\n6.2 Performance under different DP-LSTM parameter settings\nThere are many tunable parameters in DP-LSTM (e.g. the dimension of embedding vectors and the number of hidden units in the bidirectional LSTM). Due to limited space and considering the cost of training time, we only selected a few projects to demonstrate how we tune these parameters. Empirically, we first set the batch size to 32 since batch size does not affect much to the result. Then we set the dimension of embedding vectors to 16 heuristically, and gradually increase to 128 with a step of 4 until the prediction performance is worse than previous embedding dimension.", "mimetype": "text/plain", "start_char_idx": 27886, "end_char_idx": 32209, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60b087a0-10d3-458a-b540-d518291d16d4": {"__data__": {"id_": "60b087a0-10d3-458a-b540-d518291d16d4", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ed433f3-d937-46c5-9b65-152e70ee9222", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "53a9ce076c151fcb919ac9e26b6a1b5eee8afebf276a99915a905a7d1060d4a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf81715-2f6f-4b9b-8c23-99008871b88f", "node_type": "1", "metadata": {}, "hash": "e970505a199cee285037d794a63192da3a35be5e25c32c6c6bf644132a0bd67b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The average result in the last row indicates that DP-LSTM outperforms other deep learning- based methods.\n6.2 Performance under different DP-LSTM parameter settings\nThere are many tunable parameters in DP-LSTM (e.g. the dimension of embedding vectors and the number of hidden units in the bidirectional LSTM). Due to limited space and considering the cost of training time, we only selected a few projects to demonstrate how we tune these parameters. Empirically, we first set the batch size to 32 since batch size does not affect much to the result. Then we set the dimension of embedding vectors to 16 heuristically, and gradually increase to 128 with a step of 4 until the prediction performance is worse than previous embedding dimension. After an optimal embedding dimension is found, we moved on to the number of hidden units in LSTM. Higher number\n5.1 RQ1: Can DP-LSTM yield better performance than traditional methods that only use handcraft features to make predictions\nDP-LSTM, which considers semantic and contextual features, could perform better than the traditional methods that only use handcrafted features as LR can only achieve an average\n448\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n7\u2003Conclusion\nIn this paper, we proposed a novel LSTM approach to perform software defect prediction, which can automatically learn the semantic and contextual information from the program's ASTs. There are several problems remaining to be investigated in future work. We will try different ways to traverse the AST to build the AST node sequence.\n8\u2003Acknowledgment\nThe authors thank the editors and anonymous reviewers for their constructive comments and suggestions. This work was supported in part by the National Nature Science Foundation of China under grant no. 61370103, the Guangdong Province Application Major Fund under grant no. 2015B010131013 and the Guangzhou Produce & Research Fund under grant no. 201902020004.\n9\u2003References\n[1]\nKakkar, M., Jain, S., Bansal, A., et al.: \u2018Combining data preprocessing methods with imputation techniques for software defect prediction\u2019, Int. J. Open Source Softw. Processes (IJOSSP), 2018, 9, (1), pp. 1\u201319 Phan, A.V., Le Nguyen, M.: \u2018Convolutional neural networks on assembly code for predicting software defects\u2019. 2017 21st Asia Pacific Symp. on Intelligent and Evolutionary Systems (IES), Hanoi, Vietnam, 2017, pp. 37\u201342 Chug, A., Dhall, S.: \u2018Software defect prediction using supervised learning algorithm and unsupervised learning algorithm\u2019, 2013 Qiu, S., Lu, L., Jiang, S., et al.: \u2018An investigation of imbalanced ensemble learning methods for cross-project defect prediction\u2019, Int. J. Pattern Recognit. Artif. Intell., 2019, 33, (12), p. 1959037 Halstead, M.H.: \u2018Elements of software science\u2019, 1977 McCabe, T.J.: \u2018A complexity measure\u2019, IEEE Trans. Softw. Eng., 1976, SE-2, (4), pp. 308\u2013320 Chidamber, S.R., Kemerer, C.F.: \u2018A metrics suite for object oriented design\u2019, IEEE Trans. Softw. Eng., 1994, 20, (6), pp. 476\u2013493 Wang, S., Liu, T., Tan, L.: \u2018Automatically learning semantic features for defect prediction\u2019. 2016 IEEE/ACM 38th Int. Conf. on Software Engineering (ICSE), Austin, Texas, 2016, pp. 297\u2013308 Dam, H.K., Pham, T., Ng, S.W., et al.: \u2018A deep tree-based model for software defect prediction\u2019, arXiv preprint arXiv:1802.00921, 2018 He, Z., Peters, F., Menzies, T., et al.: \u2018Learning from open-source projects: an empirical study on defect prediction\u2019. 2013 ACM/IEEE Int.", "mimetype": "text/plain", "start_char_idx": 31467, "end_char_idx": 35304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bf81715-2f6f-4b9b-8c23-99008871b88f": {"__data__": {"id_": "8bf81715-2f6f-4b9b-8c23-99008871b88f", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b087a0-10d3-458a-b540-d518291d16d4", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "e04ff2ba2379c3582531bdc554c518bb25e947d1d0f1287fad0fd82db15acda1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92d8a9d2-1ef2-4a10-b365-0cd52b29241b", "node_type": "1", "metadata": {}, "hash": "4ce44ee553c863485ba55407b70e65b5aa3812d7dc4e8425e37d160788e4b0fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ": \u2018A metrics suite for object oriented design\u2019, IEEE Trans. Softw. Eng., 1994, 20, (6), pp. 476\u2013493 Wang, S., Liu, T., Tan, L.: \u2018Automatically learning semantic features for defect prediction\u2019. 2016 IEEE/ACM 38th Int. Conf. on Software Engineering (ICSE), Austin, Texas, 2016, pp. 297\u2013308 Dam, H.K., Pham, T., Ng, S.W., et al.: \u2018A deep tree-based model for software defect prediction\u2019, arXiv preprint arXiv:1802.00921, 2018 He, Z., Peters, F., Menzies, T., et al.: \u2018Learning from open-source projects: an empirical study on defect prediction\u2019. 2013 ACM/IEEE Int. Symp. on Empirical Software Engineering and Measurement, Baltimore, Maryland, USA, 2013, pp. 45\u201354 Jureczko, M., Madeyski, L.: \u2018Towards identifying software project clusters with regard to defect prediction\u2019. Proc. of the 6th Int. Conf. on Predictive Models in Software Engineering, Timi\u015foara, Romania, 2010, p. 9 Hindle, A., Barr, E.T., Su, Z., et al.: \u2018On the naturalness of software\u2019. 2012 34th Int. Conf. on Software Engineering (ICSE), Zurich, Switzerland, 2012, pp. 837\u2013847\n[2]\n[3]\nFig. 9\u2004 F1-measure of DP-ARNN under different number of embedding vector dimensions and hidden units\n[4]\nof hidden units can capture more semantic and contextual features, but too many features may lead to bad performance and longer training time. So we first start at 8 and keep increasing the number of hidden units until the performance stop to increase. We use F1- measure as the evaluation metric and finally, we calculate the average F-measure of the projects under different parameter settings. The appropriate epoch is determined by the method of early stopping.\n[5] [6]\n[7]\n[8]\n[9]\nWe select Camel, Jedit, and Lucene for parameter tuning experiments. Fig. 9 demonstrates the F-measure of DP-LSTM under the different dimensions of embedding vectors, different numbers of hidden units. The peak point of F-measure differs from projects since each project has its contextual features. So the optimal parameters might be suitable for other projects. Other parameters can also be gained via parameter adjustment.\n[10]\n[11]\n[12]\n6.3 Threats to validity\n[13] Mikolov, T., Karafi\u00e1t, M., Burget, L., et al.: \u2018Recurrent neural network based language model\u2019. Eleventh Annual Conf. of the Int. Speech Communication Association, Makuhari, Chiba, Japan, 2010 Sundermeyer, M., Schl\u00fcter, R., Ney, H.: \u2018LSTM neural networks for language modeling\u2019. Thirteenth annual Conf. of the Int. Speech Communication Association, Portland, Oregon, USA, 2012 Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., et al.: \u2018Learning phrase representations using RNN encoder-decoder for statistical machine translation\u2019, arXiv preprint arXiv:1406.1078, 2014 Graves, A., Jaitly, N., Mohamed, A.-R.: \u2018Hybrid speech recognition with deep bidirectional LSTM\u2019. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, 2013, pp. 273\u2013 278 Gao, L., Guo, Z., Zhang, H., et al.: \u2018Video captioning with attention-based LSTM and semantic consistency\u2019, IEEE Trans. Multimed., 2017, 19, (9), pp. 2045\u20132055 Li, Z., Jing, X.-Y., Zhu, X.: \u2018Progress on approaches to software defect prediction\u2019, IET Softw., 2018, 12, (3), pp. 161\u2013175 Li, J., He, P., Zhu, J., et al.: \u2018Software defect prediction via convolutional neural network\u2019. 2017 IEEE Int. Conf. on Software Quality, Reliability and Security (QRS), Olomouc, Czech Republic, 2017, pp. 318\u2013328\n6.3.1 Implementation of compared models: We compared serval deep learning-based models with our proposed DP-LSTM.", "mimetype": "text/plain", "start_char_idx": 34742, "end_char_idx": 38240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92d8a9d2-1ef2-4a10-b365-0cd52b29241b": {"__data__": {"id_": "92d8a9d2-1ef2-4a10-b365-0cd52b29241b", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf81715-2f6f-4b9b-8c23-99008871b88f", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "0a4c8c5abe4024b4213e37cca35791311da383ecd5c53898d0c49c53667bfd23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "797cce33-5648-4afe-83b3-af449a6ae5b7", "node_type": "1", "metadata": {}, "hash": "a75a534f3634151b69fd44de2c737fd25ad27705e5b334426078812e2fd0b70a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "273\u2013 278 Gao, L., Guo, Z., Zhang, H., et al.: \u2018Video captioning with attention-based LSTM and semantic consistency\u2019, IEEE Trans. Multimed., 2017, 19, (9), pp. 2045\u20132055 Li, Z., Jing, X.-Y., Zhu, X.: \u2018Progress on approaches to software defect prediction\u2019, IET Softw., 2018, 12, (3), pp. 161\u2013175 Li, J., He, P., Zhu, J., et al.: \u2018Software defect prediction via convolutional neural network\u2019. 2017 IEEE Int. Conf. on Software Quality, Reliability and Security (QRS), Olomouc, Czech Republic, 2017, pp. 318\u2013328\n6.3.1 Implementation of compared models: We compared serval deep learning-based models with our proposed DP-LSTM. Since the original implementation of DBN and DP-CNN has not yet been published, we implemented these models using Python. Thus, the results of these models might reflect all of the details in the comparison method and the parameters of these models might not be fine-tuned. To make a fair comparison, we use a consistent LR implementation and oversampling techniques and we follow our way to parse the source code into numerical vectors, which might have a difference to the original implementation of these models and we used Pytorch as our deep learning framework.\n[14]\n[15]\n[16]\n[17]\n[18]\n6.3.2 Experimental result might not be generalisable: The experimental projects all come from the PROMISE repository, which only consists of Java projects and thus, the results cannot represent all real-world projects. Since we explored the information of AST node and such information may be language-specific, there might or might not have the same or identical information that is available in other programming languages (e.g. C\u2009+\u2009+ or Python).\n[19]\n[20] Wang, S., Liu, T., Nam, J., et al.: \u2018Deep semantic feature learning for software\ndefect prediction\u2019, IEEE Trans. Softw. Eng., 2018. Early Access Bengio, Y., Simard, P., Frasconi, P., et al.: \u2018Learning long-term dependencies with gradient descent is difficult\u2019, IEEE Trans. Neural Netw., 1994, 5, (2), pp. 157\u2013166 Hochreiter, S., Schmidhuber, J.: \u2018Long short-term memory\u2019, Neural Comput., 1997, 9, (8), pp. 1735\u20131780 Liu, C., Chen, X., Shin, R., et al.: \u2018Latent attention for ifthen program synthesis\u2019. Advances in Neural Information Processing Systems, Barcelona, Spain, 2016, pp. 4574\u20134582 Reed, S., De Freitas, N.: \u2018Neural programmer-interpreters\u2019, arXiv preprint arXiv:1511.06279, 2015 Balog, M., Gaunt, A.L., Brockschmidt, M., et al.: \u2018Deepcoder: learning to write programs\u2019, arXiv preprint arXiv:1611.01989, 2016\n[21]\n[22]\n6.3.3 F-measure might not be the only appropriate measurement: In this paper, we chose F-measure as the evaluation metric of prediction models. There are many other metrics such as accuracy and G-measure that can be used for performance evaluation of a dichotomous classifier. In fact, F- measure being a comprehensive measurement is a commonly used evaluation metric in SDP.\n[23]\n[24]\n[25]\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n449\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n[26]\nc2nes: \u2018Javalang pure python java parser and tools\u2019. Available at https:// github.com/c2nes/javalang, Accessed 19 April 2019 Xiao, L., Wang, G., Zuo, Y.: \u2018Research on patent text classification based on word2vec and lstm\u2019. 2018 11th Int. Symp.", "mimetype": "text/plain", "start_char_idx": 37620, "end_char_idx": 41204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "797cce33-5648-4afe-83b3-af449a6ae5b7": {"__data__": {"id_": "797cce33-5648-4afe-83b3-af449a6ae5b7", "embedding": null, "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8578f920-f316-4f3c-a19e-4e59b34702f2", "node_type": "4", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "17f722531e499ba1bdc0040ce5395ccde55755a38831648707357a196f1153c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92d8a9d2-1ef2-4a10-b365-0cd52b29241b", "node_type": "1", "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}, "hash": "d54794d83d7902642b56c8bed4f3cb252d1ecb2195ea8a5a302a73f4ae20edbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020\n449\n17518814, 2020, 4, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0149, Wiley Online Library on [12/04/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n[26]\nc2nes: \u2018Javalang pure python java parser and tools\u2019. Available at https:// github.com/c2nes/javalang, Accessed 19 April 2019 Xiao, L., Wang, G., Zuo, Y.: \u2018Research on patent text classification based on word2vec and lstm\u2019. 2018 11th Int. Symp. Computational Intelligence and Design (ISCID), Hangzhou, China, 2019, vol. 1, pp. 71\u201374 Peng, H., Mou, L., Li, G., et al.: \u2018Building program vector representations for deep learning\u2019. Int. Conf. on Knowledge Science, Engineering and Management, Lisbon, Portugal, 2015, pp. 547\u2013553 Chen, L., Fang, B., Shang, Z., et al.: \u2018Negative samples reduction in cross- company software defects prediction\u2019, Inf. Softw. Technol., 2015, 62, pp. 67\u2013 77 Jing, X.-Y., Wu, F., Dong, X., et al.: \u2018An improved SDA based defect prediction framework for both within-project and cross-project class- imbalance problems\u2019, IEEE Trans. Softw. Eng., 2017, 43, (4), pp. 321\u2013339 Tan, M., Tan, L., Dara, S., et al.: \u2018Online defect prediction for imbalanced data\u2019. 2015 IEEE/ACM 37th IEEE Int. Conf. on Software Engineering, Florence, Italy, 2015, vol. 2, pp. 99\u2013108\n[32]\nGarc\u00eda, S., Herrera, F.: \u2018Evolutionary undersampling for classification with imbalanced datasets: proposals and taxonomy\u2019, Evol. Comput., 2014, 17, (3), pp. 275\u2013306 Dittman, D.J., Khoshgoftaar, T.M., Napolitano, A.: \u2018The effect of data sampling when using random forest on imbalanced bioinformatics data\u2019. 2015 IEEE int. Conf. on information reuse and integration, San Francisco, California, USA, 2015, pp. 457\u2013463 Su, C., Ju, S., Liu, Y., et al.: \u2018Improving random forest and rotation forest for highly imbalanced datasets\u2019, Intell. Data Anal., 2015, 19, (6), pp. 1409\u20131432 Han, J., Pei, J., Kamber, M.: \u2018Data mining: concepts and techniques\u2019 (Elsevier, Netherlands, 2011) Powers, D.M.: \u2018Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\u2019, 2011 Jian, L., He, P., Zhu, J., et al.: \u2018Software defect prediction via convolutional neural network\u2019. IEEE Int. Conf. on Software Quality, Prague, Czech Republic, 2017\n[27]\n[33]\n[28]\n[34]\n[29]\n[35]\n[30]\n[36]\n[37]\n[31]\n450\nIET Softw., 2020, Vol. 14 Iss. 4, pp. 443-450 \u00a9 The Institution of Engineering and Technology 2020", "mimetype": "text/plain", "start_char_idx": 40534, "end_char_idx": 43157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"68ec127c-3368-45d8-ba12-1ab8d1b5da7f": {"node_ids": ["afbb1e39-030f-40e0-a814-4d2a86aade92", "1a80d4cb-dd7f-4df3-9b04-ab6d7081ae56", "ea768f27-d3a3-41f5-9356-26a754f22c1a", "58d3d8b7-fad9-4ffe-9973-e328f735883a", "f4f61817-9940-491f-a132-58400ea21b8e", "2f4605df-f31e-449e-92d1-93d1274a727e", "e5a722ad-ad75-4912-bb0f-12a236644ffd", "23b822d0-f03d-4d55-9707-a849a754097d", "3fe5bdb3-cb30-4d98-bf62-5be46d14ae7d"], "metadata": {"filename": "0321.pdf"}}, "e76227a0-1079-4add-9293-ebba4cae6936": {"node_ids": ["4854438e-115d-43ce-8e87-5c2e18dedd86", "178ab083-1bba-4829-8bac-8e9f33656769", "ef6162f4-0792-4c66-830f-be31e5eceef7", "055ccaa7-9790-4892-978c-888419b70a49", "41bf9b4f-31e6-4c0b-9777-9262ea2c447e", "002cf1b1-9963-4b4e-9dc3-d013939dc585", "5c54336c-70bd-4cd5-bff0-48e4773d2172", "381082d5-7242-4bf8-88f0-c4c5bbea87e3", "f719ef32-c5f6-492b-b6b4-8de6a89f8d77"], "metadata": {"filename": "engproc-18-00030.pdf"}}, "8578f920-f316-4f3c-a19e-4e59b34702f2": {"node_ids": ["8a2722fe-d6c2-4cd5-9b74-d730cea19600", "5856ed92-9a46-4e6b-a8af-b9523f877ec8", "0bfa74d1-b907-44bf-8f95-c3f406701893", "ebb48b40-3514-4324-891c-025a784addee", "9cb55855-869d-4ba8-bf0e-8a72b410e8ca", "2d519525-935e-4fda-9d5d-a4b097534a98", "06308865-04e0-41e4-9337-1eb5aba373a4", "26538059-2366-4518-9021-dad98651bf99", "3ed433f3-d937-46c5-9b65-152e70ee9222", "60b087a0-10d3-458a-b540-d518291d16d4", "8bf81715-2f6f-4b9b-8c23-99008871b88f", "92d8a9d2-1ef2-4a10-b365-0cd52b29241b", "797cce33-5648-4afe-83b3-af449a6ae5b7"], "metadata": {"filename": "IET Software - 2020 - Deng - Software defect prediction via LSTM.pdf"}}}}